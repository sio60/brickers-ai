import json
from typing import Dict, Any
from .state import HypothesisState

# LLM Client Import (Environment setup assumed)
try:
    from ..llm_clients import GeminiClient
except ImportError:
    import sys
    import os
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../")))
    from llm_clients import GeminiClient

class HypothesisNodes:
    def __init__(self):
        self.llm = GeminiClient(model="gemini-2.5-flash") # Fast model

    def generate_hypothesis(self, state: HypothesisState) -> Dict[str, Any]:
        """
        ê°€ì„¤ ìƒì„± ë…¸ë“œ (Basic)
        """
        observation = state['observation']
        similar_cases = state.get('similar_cases', [])
        
        # 1. RAG Context Formatting
        rag_context = ""
        for case in similar_cases:
            rerank_reason = case.get('_rerank_reason', 'N/A')
            lesson = case.get('improvement', {}).get('lesson_learned', 'No lesson')
            tool = case['experiment'].get('tool')
            result = case['verification'].get('numerical_analysis', 'N/A')
            rag_context += f"- [Case: {tool}]\n  Relevance: {rerank_reason}\n  Lesson: {lesson}\n  Result: {result}\n"
            
        if not rag_context:
            rag_context = "(ìœ ì‚¬ ì‚¬ë¡€ ì—†ìŒ)"

        # 2. Prompt Construction (Basic + RAG aware)
        prompt = f"""
ë‹¹ì‹ ì€ ë ˆê³  êµ¬ì¡° ê³µí•™ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í˜„ì¬ ë°œìƒí•œ ë¬¼ë¦¬ì  ê²€ì¦ ì‹¤íŒ¨ì˜ ì›ì¸ì„ ë¶„ì„í•˜ê³  í•´ê²° ê°€ì„¤ì„ ìˆ˜ë¦½í•˜ì„¸ìš”.

[í˜„ì¬ ìƒí™© (Observation)]
{observation}
- Metrics: {state.get('verification_result', {})}

[ìœ ì‚¬ ê³¼ê±° ì‚¬ë¡€ (RAG Context)]
{rag_context}

[ì§€ì¹¨]
1. ì‹¤íŒ¨ì˜ 'ê·¼ë³¸ ì›ì¸(Root Cause)'ì„ ë¶„ì„í•˜ì„¸ìš”.
2. ê³¼ê±° ì‚¬ë¡€ì˜ êµí›ˆ(Lesson)ì„ ì°¸ê³ í•˜ì—¬ ê°€ì¥ ì„±ê³µ í™•ë¥ ì´ ë†’ì€ ì ‘ê·¼ë²•ì„ ë„ì¶œí•˜ì„¸ìš”.
3. êµ¬ì²´ì ì¸ ê°€ì„¤ì„ ìˆ˜ë¦½í•˜ì„¸ìš”.

[ì‘ë‹µ í¬ë§· (JSON)]
{{
    "observation": "í˜„ì¬ ë¬¸ì œ ìƒí™© ìš”ì•½ (1ë¬¸ì¥)",
    "root_cause": "ë¬¼ë¦¬ì  ì‹¤íŒ¨ ì›ì¸ ë¶„ì„",
    "hypothesis": "êµ¬ì²´ì ì¸ í•´ê²° ê°€ì„¤ (If-Then-Because)",
    "reasoning": "ê°€ì„¤ì˜ ê·¼ê±°",
    "difficulty": "Easy|Medium|Hard"
}}
"""
        try:
            response = self.llm.generate_json(prompt)
            print(f"  ğŸ’­ [HypothesisMaker] ê°€ì„¤ ìƒì„± ì™„ë£Œ: {response.get('hypothesis')}")
            return {"final_hypothesis": response}
            
        except Exception as e:
            print(f"  âš ï¸ [HypothesisMaker] ê°€ì„¤ ìƒì„± ì‹¤íŒ¨: {e}")
            fallback = {
                "observation": "ë¶„ì„ ì‹¤íŒ¨",
                "hypothesis": "ê¸°ë³¸ ì „ëµ ìœ ì§€ (ìƒì„± ì‹¤íŒ¨)",
                "difficulty": "Medium"
            }
            return {"final_hypothesis": fallback}
