# ============================================================================
# LLM ì¬ìƒì„± ì—ì´ì „íŠ¸ (Tool Calling & History ê¸°ë°˜)
# GLB â†’ LDR ë³€í™˜ í›„ ë¬¼ë¦¬ ê²€ì¦ ì‹¤íŒ¨ ì‹œ LLMì´ 'ë„êµ¬'ë¥¼ ì‚¬ìš©í•´ í•´ê²°ì±…ì„ ì œì‹œí•˜ëŠ” ì‹œìŠ¤í…œ
#
# ì•„í‚¤í…ì²˜ (LangGraph):
# 1. Generator Node: GLB ë³€í™˜ (TuneParameters ë„êµ¬ ê²°ê³¼ ë°˜ì˜)
# 2. Verifier Node: ë¬¼ë¦¬ ê²€ì¦ ë° ê²°ê³¼ í”¼ë“œë°± ìƒì„± (Historyì— ì¶”ê°€)
# 3. Model Node: LLMì´ Historyë¥¼ ë³´ê³  TuneParameters ë„êµ¬ë¡œ íŒŒë¼ë¯¸í„° ì¡°ì •
# 4. Tool Node: ì„ íƒëœ ë„êµ¬ ì‹¤í–‰ (ì•Œê³ ë¦¬ì¦˜ ì¬ìƒì„±)
# ============================================================================

import sys
import os
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Literal, TypedDict, Union
from dataclasses import dataclass, field, asdict
import json

# LangGraph & LangChain imports
try:
    from langgraph.graph import StateGraph, END
    from langgraph.graph.message import add_messages
    from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage, ToolMessage
    from typing import Annotated
except ImportError:
    print("âŒ LangGraph ë˜ëŠ” LangChainì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install langgraph langchain-core'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    sys.exit(1)

# ëª¨ë“ˆ ê²½ë¡œ ì„¤ì •
_THIS_DIR = Path(__file__).resolve().parent
_BRICK_ENGINE_DIR = _THIS_DIR.parent
_PROJECT_ROOT = _BRICK_ENGINE_DIR.parent
_PHYSICAL_VERIFICATION_DIR = _PROJECT_ROOT / "physical_verification"

for p in (_THIS_DIR, _BRICK_ENGINE_DIR, _PROJECT_ROOT, _PHYSICAL_VERIFICATION_DIR):
    sp = str(p)
    if sp not in sys.path:
        sys.path.insert(0, sp)


# LLM í´ë¼ì´ì–¸íŠ¸ & ë„êµ¬ ì„í¬íŠ¸
try:
    from .llm_clients import BaseLLMClient, GroqClient, GeminiClient
    from .agent_tools import TuneParameters
    from .memory_utils import memory_manager, build_hypothesis, build_experiment, build_verification, build_improvement
except ImportError:
    from llm_clients import BaseLLMClient, GroqClient, GeminiClient
    from agent_tools import TuneParameters
    from memory_utils import memory_manager, build_hypothesis, build_experiment, build_verification, build_improvement

# DB ì—°ê²°
try:
    from yang_db import get_db
except ImportError:
    print("âš ï¸ yang_db.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Memory ì˜ì†í™” ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
    get_db = None

# ============================================================================
# Memory & DB Helper Functions
# ============================================================================

try:
    from memory_utils import memory_manager, build_hypothesis, build_experiment, build_verification, build_improvement
except ImportError:
    # ê²½ë¡œ ë¬¸ì œ ì‹œ í˜„ì¬ í´ë” ì¶”ê°€ í›„ ì¬ì‹œë„
    sys.path.append(str(_THIS_DIR))
    try:
        from memory_utils import memory_manager, build_hypothesis, build_experiment, build_verification, build_improvement
    except ImportError:
        print("âš ï¸ memory_utils.pyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        memory_manager = None
        build_hypothesis = build_experiment = build_verification = build_improvement = None

# Legacy functions (í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ë‚¨ê²¨ë‘ê±°ë‚˜ ì‚­ì œ ê°€ëŠ¥)
def get_memory_collection(): return memory_manager.collection_exps if memory_manager else None
def load_memory_from_db(model_id: str): return {} # Legacy ë¡œë“œ ë¹„í™œì„±í™” (RAGë¡œ ëŒ€ì²´)
def save_memory_to_db(model_id: str, memory: Dict): pass # Legacy ì €ì¥ ë¹„í™œì„±í™”


# ============================================================================
# ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì •ì˜
# ============================================================================

DEFAULT_PARAMS = {
    "target": 60,              # ëª©í‘œ ìŠ¤í„°ë“œ í¬ê¸° (400 ë¸Œë¦­ ê¸°ì¤€ 60 ì •ë„ê°€ ì ì ˆ)
    "min_target": 5,           # ìµœì†Œ ìŠ¤í„°ë“œ í¬ê¸°
    "budget": 400,             # ìµœëŒ€ ë¸Œë¦­ ìˆ˜ (Kids L1 ê¸°ì¤€)
    "shrink": 0.85,            # ì¶•ì†Œ ë¹„ìœ¨ (0.85)
    "search_iters": 6,         # ì´ì§„ íƒìƒ‰ ë°˜ë³µ íšŸìˆ˜
    "flipx180": False,         # Xì¶• 180ë„ íšŒì „
    "flipy180": False,         # Yì¶• 180ë„ íšŒì „
    "flipz180": False,         # Zì¶• 180ë„ íšŒì „
    "kind": "brick",           # ë¸Œë¦­ ì¢…ë¥˜ (brick/plate)
    "plates_per_voxel": 3,     # ë³µì…€ë‹¹ í”Œë ˆì´íŠ¸ ìˆ˜
    "interlock": True,         # ì¸í„°ë½ í™œì„±í™”
    "max_area": 20,            # ìµœëŒ€ ì˜ì—­
    "solid_color": 4,          # ë‹¨ìƒ‰ ìƒ‰ìƒ ID
    "use_mesh_color": True,    # ë©”ì‹œ ìƒ‰ìƒ ì‚¬ìš©
    "invert_y": False,         # Yì¶• ë°˜ì „
    "smart_fix": True,         # ìŠ¤ë§ˆíŠ¸ ë³´ì • í™œì„±í™”
    # ì¶”ê°€ íŒŒë¼ë¯¸í„° (Legacy Match)
    "span": 4,
    "max_new_voxels": 12000,
    "refine_iters": 8,
    "ensure_connected": True,
    "min_embed": 2,
    "erosion_iters": 1,        # ë…¸ì´ì¦ˆ ì œê±°
    "fast_search": True,
    "extend_catalog": True,
    "max_len": 8,
    "fill": True,              # ë‚´ë¶€ ì±„ì›€ í™œì„±í™” (ì•ˆì •ì  ëª¨ë¸ ìƒì„±)
    "step_order": "bottomup",  # ì¡°ë¦½ ìˆœì„œ
    "auto_remove_1x1": True,   # ê¸°ë³¸ê°’: ì•ˆì „í•˜ê²Œ 1x1 ì‚­ì œ
    "support_ratio": 0.3,      # ê¸°ë³¸ ì§€ì§€ ë¹„ìœ¨ ë³µêµ¬
    "small_side_contact": True, # ì‘ì€ ë¸Œë¦­ ì‚¬ì´ë“œ ì ‘ì´‰ í—ˆìš©
}


# ============================================================================
# ë°ì´í„° êµ¬ì¡° ë° í—¬í¼ í•¨ìˆ˜
# ============================================================================

@dataclass
class VerificationFeedback:
    """PyBullet ê²€ì¦ ê²°ê³¼ë¥¼ LLMì—ê²Œ ì „ë‹¬í•˜ê¸° ìœ„í•œ êµ¬ì¡°í™”ëœ í”¼ë“œë°±"""
    stable: bool = True
    total_bricks: int = 0
    fallen_bricks: int = 0
    floating_bricks: int = 0
    floating_brick_ids: List[str] = field(default_factory=list)  # ê³µì¤‘ë¶€ì–‘ ë¸Œë¦­ ID ëª©ë¡
    fallen_brick_ids: List[str] = field(default_factory=list)    # ë–¨ì–´ì§„ ë¸Œë¦­ ID ëª©ë¡
    failure_ratio: float = 0.0
    first_failure_brick: Optional[str] = None
    max_drift: float = 0.0
    collision_count: int = 0
    # ì•ˆì •ì„± ë“±ê¸‰ (3ë‹¨ê³„)
    stability_grade: str = "STABLE"       # "STABLE" | "MEDIUM" | "UNSTABLE"
    stability_score: int = 100            # 0~100
    # 1x1 ë¸Œë¦­ ë¹„ìœ¨ ì •ë³´
    small_brick_count: int = 0      # 1x1 ë¸Œë¦­ ê°œìˆ˜
    small_brick_ratio: float = 0.0  # 1x1 ë¸Œë¦­ ë¹„ìœ¨ (0.0 ~ 1.0)

def extract_verification_feedback(result, total_bricks: int) -> VerificationFeedback:
    """PyBullet VerificationResultë¥¼ LLM í”¼ë“œë°± í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    feedback = VerificationFeedback()
    feedback.total_bricks = total_bricks
    feedback.stable = result.is_valid
    
    fallen_bricks = set()
    floating_bricks = set()
    first_failure = None
    collision_count = 0
    
    for ev in result.evidence:
        if ev.type == "FIRST_FAILURE":
            if ev.brick_ids:
                first_failure = ev.brick_ids[0]
                fallen_bricks.update(ev.brick_ids)
        elif ev.type == "COLLAPSE_AFTERMATH":
            if ev.brick_ids:
                fallen_bricks.update(ev.brick_ids)
        elif ev.type in ("FLOATING_BRICK", "FLOATING"):
            if ev.brick_ids:
                floating_bricks.update(ev.brick_ids)
        elif ev.type == "COLLISION":
            collision_count += 1
    
    feedback.fallen_bricks = len(fallen_bricks)
    feedback.floating_bricks = len(floating_bricks)
    feedback.floating_brick_ids = list(floating_bricks)  # ID ëª©ë¡ ì €ì¥
    feedback.fallen_brick_ids = list(fallen_bricks)      # ID ëª©ë¡ ì €ì¥
    feedback.first_failure_brick = first_failure
    feedback.collision_count = collision_count
    
    if total_bricks > 0:
        feedback.failure_ratio = (len(fallen_bricks) + len(floating_bricks)) / total_bricks

    # ì•ˆì •ì„± ë“±ê¸‰ ì¶”ì¶œ (PyBullet VerificationResultì—ì„œ)
    feedback.stability_grade = getattr(result, 'stability_grade', 'STABLE')
    feedback.stability_score = int(getattr(result, 'score', 100))

    return feedback

def _format_feedback(feedback: VerificationFeedback) -> str:
    GRADE_EMOJI = {"STABLE": "ğŸŸ¢", "MEDIUM": "ğŸŸ¡", "UNSTABLE": "ğŸ”´"}
    GRADE_LABEL = {"STABLE": "ì•ˆì •", "MEDIUM": "ì¤‘ê°„", "UNSTABLE": "ë¶ˆì•ˆì •"}

    grade = feedback.stability_grade
    emoji = GRADE_EMOJI.get(grade, "âšª")
    label = GRADE_LABEL.get(grade, grade)

    lines = [
        f"ê²€ì¦ ê²°ê³¼:",
        f"- {emoji} ì•ˆì •ì„± ë“±ê¸‰: {label} ({grade})",
        f"- ğŸ“Š ì ìˆ˜: {feedback.stability_score}/100",
        f"- ì´ ë¸Œë¦­ ìˆ˜: {feedback.total_bricks}ê°œ",
        f"- ìµœëŒ€ ë³€ìœ„(Drift): {feedback.max_drift:.3f}",
    ]

    # 1x1 ë¸Œë¦­ ë¹„ìœ¨ ì •ë³´
    if feedback.small_brick_count > 0:
        lines.append(f"- 1x1 ë¸Œë¦­: {feedback.small_brick_count}ê°œ ({feedback.small_brick_ratio * 100:.1f}%)")

    # ìƒì„¸ ë¶ˆì•ˆì • ì •ë³´
    if grade != "STABLE" or feedback.floating_bricks > 0:
        lines.extend([
            f"- ë–¨ì–´ì§„ ë¸Œë¦­: {feedback.fallen_bricks}ê°œ",
            f"- ê³µì¤‘ë¶€ì–‘ ë¸Œë¦­: {feedback.floating_bricks}ê°œ",
            f"- ì‹¤íŒ¨ìœ¨: {feedback.failure_ratio * 100:.1f}%",
        ])
        if feedback.first_failure_brick:
            lines.append(f"- ìµœì´ˆ ë¶•ê´´ ë¸Œë¦­: {feedback.first_failure_brick}")

    if feedback.collision_count > 0:
        lines.append(f"- ì¶©ëŒ ê°ì§€: {feedback.collision_count}ê±´")

    return "\n".join(lines)


# ============================================================================
# LangGraph State ì •ì˜
# ============================================================================

class AgentState(TypedDict):
    # ì…ë ¥ ë° ì„¤ì •
    glb_path: str
    ldr_path: str
    subject_name: str          # [ì¶”ê°€] ì‚¬ë¬¼ì˜ ì •ì²´ì„± (ì˜ˆ: "ê°•ì•„ì§€", "ìë™ì°¨")
    params: Dict[str, Any]
    max_retries: int
    acceptable_failure_ratio: float
    verification_duration: float
    gui: bool
    
    # ì‹¤í–‰ ìƒíƒœ
    attempts: int
    session_id: str # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ìš© ì„¸ì…˜ ID
    messages: Annotated[List[BaseMessage], add_messages] # ëŒ€í™” ê¸°ë¡ (History)
    
    # ê²€ì¦ ê²°ê³¼ ìºì‹œ (Tool ì‹¤í–‰ ì‹œ ì°¸ì¡°ìš©)
    verification_raw_result: Any 
    floating_bricks_ids: List[str] # ê³µì¤‘ë¶€ì–‘ ë¸Œë¦­ ID ëª©ë¡ ìºì‹œ
    verification_errors: int  # ê²€ì¦ ì—ëŸ¬ ì¬ì‹œë„ ì¹´ìš´í„°

    # ë¬´í•œ ë£¨í”„ ë°©ì§€ìš© ë„êµ¬ ì‚¬ìš© ì¶”ì 
    tool_usage_count: Dict[str, int]  # {"TuneParameters": 2, ...}
    last_tool_used: Optional[str]     # ë§ˆì§€ë§‰ ì‚¬ìš© ë„êµ¬
    consecutive_same_tool: int        # ê°™ì€ ë„êµ¬ ì—°ì† ì‚¬ìš© íšŸìˆ˜
    
    # ë„êµ¬ íš¨ê³¼ ì¸¡ì •ìš© ìƒíƒœ ì €ì¥
    previous_metrics: Dict[str, Any]  # ë„êµ¬ ì‹¤í–‰ ì „ ë©”íŠ¸ë¦­
    current_metrics: Dict[str, Any]   # ê²€ì¦ í›„ í˜„ì¬ ë©”íŠ¸ë¦­ (Reflectì—ì„œ ë¹„êµìš©)
    
    # ìµœì¢… ê²°ê³¼ ë¦¬í¬íŠ¸
    final_report: Dict[str, Any]  # ìµœì¢… ê²°ê³¼ ìš”ì•½
    
    # Co-Scientist Memory (í•™ìŠµ ë©”ëª¨ë¦¬)
    memory: Dict[str, Any]  # {
    #     "failed_approaches": ["TuneParameters with target=80 failed"],
    #     "successful_patterns": ["interlock=True with fill=True"],
    #     "lessons": ["support_ratio 0.5 ì´ìƒì—ì„œ ì•ˆì •ì„± í–¥ìƒ"],
    #     "consecutive_failures": 0
    # }

    # [v2] Co-Scientist ì•„í‚¤í…ì²˜ ì¶”ê°€ í•„ë“œ
    current_hypothesis: Optional[Dict[str, Any]]  # node_hypothesize ê²°ê³¼
    strategy_plan: Optional[Dict[str, Any]]       # node_strategy ê²°ê³¼
    llm_config: Optional[Dict[str, str]]          # {"model": "gpt-4o"}

    # ë‹¤ìŒ ë…¸ë“œ ì œì–´
    next_action: Literal["generate", "verify", "model", "tool", "reflect", "hypothesize", "strategy", "end"]
    
    # [ì¶”ê°€] ì‚¬ìš©ì ì‹ë³„ ì •ë³´ (ë¡œê¹…ìš©)
    user_email: str


# ============================================================================
# LangGraph Agent Logic
# ============================================================================

class RegenerationGraph:
    # ì´ˆê¸° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (Tool ì‚¬ìš© ê¶Œì¥)
    SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ë ˆê³  ë¸Œë¦­ êµ¬ì¡°ë¬¼ ì„¤ê³„ ë° ì•ˆì •í™” ì „ë¬¸ê°€(Co-Scientist)ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ 3D ëª¨ë¸(GLB)ì„ ë ˆê³ (LDR)ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” êµ¬ì¡°ì  ë¶ˆì•ˆì •ì„± ë¬¸ì œë¥¼ í•´ê²°í•´ì•¼ í•©ë‹ˆë‹¤.

**í•µì‹¬ ì›ì¹™: LLMì€ ê²€ì¦/ë¶„ì„ë§Œ, ê°œì„ ì€ ì•Œê³ ë¦¬ì¦˜ì´ ë‹´ë‹¹í•©ë‹ˆë‹¤.**
- ë‹¹ì‹ ì€ LDR íŒŒì¼ì„ ì§ì ‘ ìˆ˜ì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ë‹¹ì‹ ì€ ê²€ì¦ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³ , íŒŒë¼ë¯¸í„° ì¡°ì •ì„ í†µí•´ ì•Œê³ ë¦¬ì¦˜ì´ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë§Œë“¤ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.

ë‹¹ì‹ ì—ê²ŒëŠ” í•˜ë‚˜ì˜ ë„êµ¬ê°€ ìˆìŠµë‹ˆë‹¤:
- `TuneParameters`: ë³€í™˜ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì—¬ ì•Œê³ ë¦¬ì¦˜ì´ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ìƒì„±í•©ë‹ˆë‹¤.

**ì•ˆì •ì„± ë“±ê¸‰ (Stability Grade):**
- ğŸŸ¢ STABLE (ì•ˆì •, 90~100ì ): ëƒ…ë’€ì„ ë•Œ ì˜ ì„œìˆìŒ - íŒŒë¼ë¯¸í„° ê·¸ëŒ€ë¡œ ìœ ì§€
- ğŸŸ¡ MEDIUM (ì¤‘ê°„, 40~89ì ): ëƒ…ë’€ì„ ë•Œ ê¸°ìš°ëŠ” ì •ë„ - íŒŒë¼ë¯¸í„° ì†Œí­ ì¡°ì • í•„ìš”
- ğŸ”´ UNSTABLE (ë¶ˆì•ˆì •, 0~39ì ): ëƒ…ë’€ì„ ë•Œ ë¬´ë„ˆì§ - íŒŒë¼ë¯¸í„° ëŒ€í­ ë³€ê²½ í•„ìš”

**ì˜ì‚¬ê²°ì • ì•Œê³ ë¦¬ì¦˜ (Decision Logic):**
1. **STABLE (ì•ˆì •)**: ì„±ê³µì…ë‹ˆë‹¤. ì¶”ê°€ ì¡°ì • ë¶ˆí•„ìš”.
2. **MEDIUM (ì¤‘ê°„)**: ì†Œí­ ì¡°ì •ìœ¼ë¡œ ê°œì„  ê°€ëŠ¥. interlock, fill, support_ratio ë“±ì„ ë¯¸ì„¸ ì¡°ì •í•˜ì„¸ìš”.
3. **UNSTABLE (ë¶ˆì•ˆì •)**: target, budget ë“± í•µì‹¬ íŒŒë¼ë¯¸í„°ë¥¼ ë³€ê²½í•˜ì—¬ ì¬ìƒì„±í•˜ì„¸ìš”.

ëª©í‘œ: ë¬¼ë¦¬ì ìœ¼ë¡œ ì•ˆì •ì (STABLE)ì¸ ë ˆê³  êµ¬ì¡°ë¬¼ì„ ë§Œë“œëŠ” ê²ƒ.
ì´ì „ ì‹œë„ì˜ ê²€ì¦ ê²°ê³¼(ì•ˆì •ì„± ë“±ê¸‰, ì ìˆ˜, ì‹¤íŒ¨ìœ¨)ë¥¼ ë¶„ì„í•˜ê³  íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì„¸ìš”.
"""

    def __init__(self, llm_client: Optional[BaseLLMClient] = None, log_callback=None):
        # ê¸°ë³¸ í´ë¼ì´ì–¸íŠ¸ëŠ” Gemini (ë¹„ìš© íš¨ìœ¨ì„±)
        self.gemini_client = GeminiClient()
        self.default_client = llm_client if llm_client else self.gemini_client
        
        # [Rollback] GPT ClientëŠ” í˜„ì¬ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (User Request)
        self.gpt_client = None
            
        # SSE ë¡œê·¸ ì½œë°± (Kids ëª¨ë“œìš©)
        self._log_callback = log_callback
        self.verifier = None
        self.user_email = "System"

    def _log(self, step: str, message: str):
        """SSE ë¡œê·¸ ì „ì†¡ í—¬í¼"""
        # ì„œë²„ ë¡œê·¸(stdout)ìš© íƒ€ì„ìŠ¤íƒ¬í”„ ë¡œê¹… (user_email í¬í•¨)
        from datetime import datetime
        ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
        user_tag = f"[{self.user_email}]" if self.user_email else "[System]"
        print(f"[{ts}] {user_tag} [Agent:{step}] {message}")

        if self._log_callback:
            try:
                self._log_callback(step, message)
            except Exception:
                pass  # fire-and-forget
        
    # --- Nodes ---

    def _rerank_and_filter_cases(self, observation: str, cases: List[Dict]) -> List[Dict]:
        """[ì‹ ê·œ] LLM ê¸°ë°˜ RAG Re-ranking (Semantic Scoring)"""
        if not cases:
            return []
            
        print(f"  ğŸ” Re-ranking: {len(cases)}ê°œ í›„ë³´ ë¶„ì„ ì¤‘...")
        
        # í›„ë³´êµ° í…ìŠ¤íŠ¸ ë³€í™˜
        candidates_text = ""
        for i, case in enumerate(cases):
            candidates_text += f"""
[Case {i}]
- Observation: {case['hypothesis'].get('observation', '')[:200]}...
- Action: {case['experiment'].get('tool', '')}
- Result: {case['verification'].get('numerical_analysis', '')}
--------------------------------------------------"""

        prompt = f"""
í˜„ì¬ ìƒí™©(Current Observation)ê³¼ ê°€ì¥ ì „ëµì ìœ¼ë¡œ ìœ ì‚¬í•œ ê³¼ê±° ì‚¬ë¡€ë¥¼ ì„ ë³„í•˜ì„¸ìš”.
ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­ì´ ì•„ë‹ˆë¼, 'ì‹¤íŒ¨/ì„±ê³µ ì›ì¸'ê³¼ 'êµ¬ì¡°ì  ë¬¸ì œ'ê°€ ìœ ì‚¬í•œì§€ ë¶„ì„í•´ì•¼ í•©ë‹ˆë‹¤.

í˜„ì¬ ìƒí™©:
{observation}

í›„ë³´ ì‚¬ë¡€ ëª©ë¡:
{candidates_text}

ë¶„ì„ í›„ ê°€ì¥ ì°¸ê³  ê°€ì¹˜ê°€ ë†’ì€ Top 3 ì‚¬ë¡€ë¥¼ ë‹¤ìŒ JSON í¬ë§·ìœ¼ë¡œ ì„ ì •í•˜ì„¸ìš”:
{{
    "top_cases": [
        {{
            "case_index": 0,
            "relevance_score": 0.95, (0.0~1.0)
            "reason": "í˜„ì¬ ìƒí™©(~í•œ ë¬¸ì œ)ê³¼ ë™ì¼í•œ ì‹¤íŒ¨ íŒ¨í„´ì„ ë³´ì„"
        }},
        ...
    ]
}}
"""
        try:
            response = self.default_client.generate_json(prompt)
            top_cases = response.get("top_cases", [])
            
            # ì¸ë±ìŠ¤ë¡œ ì›ë³¸ ì°¾ì•„ì„œ ë°˜í™˜ (ì ìˆ˜ìˆœ ì •ë ¬)
            reranked_results = []
            for item in sorted(top_cases, key=lambda x: x.get('relevance_score', 0), reverse=True):
                idx = item.get("case_index")
                if 0 <= idx < len(cases):
                    case = cases[idx]
                    # ë©”íƒ€ë°ì´í„°ì— Re-ranking ì ìˆ˜ì™€ ì´ìœ  ì¶”ê°€
                    case['_rerank_score'] = item.get('relevance_score')
                    case['_rerank_reason'] = item.get('reason')
                    reranked_results.append(case)
            
            print(f"  âœ¨ Re-ranking ì™„ë£Œ: Top {len(reranked_results)} ì„ ì • (Max Score: {reranked_results[0]['_rerank_score'] if reranked_results else 0})")
            return reranked_results
            
        except Exception as e:
            print(f"  âš ï¸ Re-ranking ì‹¤íŒ¨ (Fallback to raw vector rank): {e}")
            return cases[:3]  # ì‹¤íŒ¨ ì‹œ ê·¸ëƒ¥ ë²¡í„° ìƒìœ„ 3ê°œ ë°˜í™˜

    def node_hypothesize(self, state: AgentState) -> Dict[str, Any]:
        """[ì‹ ê·œ] ê°€ì„¤ ìƒì„± ë…¸ë“œ: RAG ê²€ìƒ‰ ë° êµ¬ì²´ì  ê°€ì„¤ ìˆ˜ë¦½"""
        print("\n[Hypothesize] ê°€ì„¤ ìˆ˜ë¦½ ë° RAG ê²€ìƒ‰ ì¤‘...")
        self._log("HYPOTHESIZE", "ìœ ì‚¬ ì‚¬ë¡€ë¥¼ ê²€ìƒ‰í•˜ê³  ê°€ì„¤ì„ ìˆ˜ë¦½í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        # 1. RAG ê²€ìƒ‰
        current_observation = ""
        last_msg = state['messages'][-1]
        if isinstance(last_msg, HumanMessage):
            current_observation = str(last_msg.content)[:500]
            
        similar_cases = []
        if memory_manager:
            # 1. ë„“ì€ ë²”ìœ„ ê²€ìƒ‰ (Top 10) - ë©”íŠ¸ë¦­ ì •ë³´ í¬í•¨í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ
            verification_metrics = state.get("verification_result")
            raw_cases = memory_manager.search_similar_cases(
                current_observation, 
                limit=10, 
                min_score=0.5,
                verification_metrics=verification_metrics
            )
            # 2. LLM Re-ranking (Top 3 ì„ ë³„)
            similar_cases = self._rerank_and_filter_cases(current_observation, raw_cases)
            print(f"  ğŸ“š ìœ ì‚¬ ì‹¤íŒ¨ ì‚¬ë¡€ {len(similar_cases)}ê±´ ì„ ì • (Re-ranked)")
            
        # 2. ê°€ì„¤ ìƒì„± (Gemini Fast ì‚¬ìš©)
        rag_context = ""
        for case in similar_cases:
            rag_context += f"- {case.get('model_id')}: {case['experiment'].get('tool')} ì‚¬ìš© -> {case['verification'].get('numerical_analysis')}\n"
            
        prompt = f"""
í˜„ì¬ ìƒí™©:
{current_observation}

ìœ ì‚¬ ê³¼ê±° ì‚¬ë¡€:
{rag_context}

ìœ„ ìƒí™©ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ê°€ì„¤ì„ ìˆ˜ë¦½í•˜ì„¸ìš”:
{{
    "observation": "í˜„ì¬ ë¬¸ì œ ìƒí™© ìš”ì•½ (1ë¬¸ì¥)",
    "hypothesis": "êµ¬ì²´ì ì¸ í•´ê²° ê°€ì„¤ (ì–´ë–¤ ë„êµ¬ê°€ ì™œ íš¨ê³¼ì ì¼ì§€)",
    "reasoning": "ê°€ì„¤ì˜ ê·¼ê±° (ê³¼ê±° ì‚¬ë¡€ ë˜ëŠ” ë…¼ë¦¬ì  ì¶”ë¡ )",
    "difficulty": "Easy|Medium|Hard" (ë¬¸ì œ ë‚œì´ë„ í‰ê°€)
}}
"""
        try:
            # ê°€ì„¤ ìƒì„±ì€ ë¹ ë¥¸ Gemini ì‚¬ìš©
            response = self.gemini_client.generate_json(prompt)
            print(f"  ğŸ’­ ê°€ì„¤: {response.get('hypothesis')}")
            print(f"  ğŸ“Š ë‚œì´ë„: {response.get('difficulty')}")
            
            return {
                "current_hypothesis": response,
                "next_action": "strategy"
            }
        except Exception as e:
            print(f"  âš ï¸ ê°€ì„¤ ìƒì„± ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê°€ì„¤ë¡œ ì§„í–‰
            return {
                "current_hypothesis": {"observation": "ë¶„ì„ ì‹¤íŒ¨", "difficulty": "Medium"},
                "next_action": "strategy"
            }

    def node_strategy(self, state: AgentState) -> Dict[str, Any]:
        """[ì‹ ê·œ] ì „ëµ ê²°ì • ë…¸ë“œ: ë‚œì´ë„ì— ë”°ë¥¸ LLM ëª¨ë¸ ì„ íƒ"""
        self._log("STRATEGY", "ìµœì ì˜ ì „ëµì„ ê²°ì •í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        hypothesis = state.get("current_hypothesis", {})
        difficulty = hypothesis.get("difficulty", "Medium")
        
        # [Rollback] GPT ì‚¬ìš© ì•ˆ í•¨ -> ë¬´ì¡°ê±´ Gemini ì„ íƒ
        model_selection = "gemini-2.5-flash"
        
        if difficulty == "Hard":
             reason = "ë‚œì´ë„ ë†’ìŒ (Hard) - Gemini ì§‘ì¤‘ ëª¨ë“œ ê¶Œì¥"
        elif difficulty == "Easy":
            reason = "ë‚œì´ë„ ë‚®ìŒ (Easy)"
        else:
            reason = "ì¼ë°˜ ë‚œì´ë„"
                
        print(f"\n[Strategy] ì „ëµ ê²°ì •: {model_selection} ({reason})")
        
        return {
            "llm_config": {"model": model_selection},
            "strategy_plan": {"selected_model": model_selection, "reason": reason},
            "next_action": "model"
        }

    def node_generator(self, state: AgentState) -> Dict[str, Any]:
        """GLB -> LDR ë³€í™˜ ë…¸ë“œ"""
        from glb_to_ldr_embedded import convert_glb_to_ldr
        
        print(f"\n[Generator] ë³€í™˜ ì‹œë„ {state['attempts'] + 1}/{state['max_retries']}")
        self._log("GENERATE", f"ë¸Œë¦­ ëª¨ë¸ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤... (ì‹œë„ {state['attempts'] + 1}/{state['max_retries']})")
        print(f"  Params: target={state['params'].get('target')}, budget={state['params'].get('budget')}")
        
        try:
            # ê³ ìˆ˜ì¤€ API í˜¸ì¶œ (ë‚´ë¶€ì ìœ¼ë¡œ budget-finding ë£¨í”„ í¬í•¨)
            result = convert_glb_to_ldr(
                state['glb_path'],
                state['ldr_path'],
                **state['params']
            )
            
            brick_count = result.get('parts', 0)
            final_target = result.get('final_target', 0)
            
            print(f"  âœ… ë³€í™˜ ì™„ë£Œ: {brick_count}ê°œ ë¸Œë¦­ (Final Target: {final_target})")
            return {"attempts": state['attempts'] + 1, "next_action": "verify"}
            
        except Exception as e:
            print(f"  âŒ ë³€í™˜ ì‹¤íŒ¨: {e}")
            # ë³€í™˜ ìì²´ê°€ ì‹¤íŒ¨í•˜ë©´ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ historyì— ì¶”ê°€í•˜ê³  Modelì—ê²Œ ë„ì›€ ìš”ì²­
            error_msg = f"ë³€í™˜ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}. íŒŒë¼ë¯¸í„°ë¥¼ í¬ê²Œ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤."
            return {
                "attempts": state['attempts'] + 1,
                "messages": [HumanMessage(content=error_msg)],
                "next_action": "model"
            }

    def node_verifier(self, state: AgentState) -> Dict[str, Any]:
        """ë¬¼ë¦¬ ê²€ì¦ ë…¸ë“œ"""
        from physical_verification.ldr_loader import LdrLoader
        from physical_verification.verifier import PhysicalVerifier

        print("\n[Verifier] ë¬¼ë¦¬ ê²€ì¦ ìˆ˜í–‰ ì¤‘...")
        self._log("VERIFY", "ë¬¼ë¦¬ ì•ˆì •ì„±ì„ ê²€ì¦í•˜ê³  ìˆìŠµë‹ˆë‹¤...")

        if not os.path.exists(state['ldr_path']):
            return {"messages": [HumanMessage(content="LDR íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")], "next_action": "model"}

        try:
            loader = LdrLoader()
            plan = loader.load_from_file(state['ldr_path'])
            total_bricks = len(plan.bricks)

            # 1x1 ë¸Œë¦­ ë¹„ìœ¨ ê³„ì‚°
            small_brick_parts = {"3005.dat", "3024.dat"}  # 1x1 ë¸Œë¦­, 1x1 í”Œë ˆì´íŠ¸
            small_brick_count = 0
            for b in plan.bricks:
                # ë¸Œë¦­ ê°ì²´ì˜ part_id ì†ì„± ì•ˆì „í•˜ê²Œ ì ‘ê·¼
                part_id = getattr(b, 'part_id', None) or (b.get('part') if isinstance(b, dict) else None)
                if part_id in small_brick_parts:
                    small_brick_count += 1
            small_brick_ratio = small_brick_count / total_bricks if total_bricks > 0 else 0.0

            # PhysicalVerifierë¡œ ë¬¼ë¦¬ ê²€ì¦ ìˆ˜í–‰
            verifier = PhysicalVerifier(plan)
            self.verifier = verifier

            stab_result = verifier.run_stability_check()
            
            feedback = extract_verification_feedback(stab_result, total_bricks)
            # 1x1 ë¸Œë¦­ ë¹„ìœ¨ ì •ë³´ ì¶”ê°€
            feedback.small_brick_count = small_brick_count
            feedback.small_brick_ratio = small_brick_ratio
            
            feedback_text = _format_feedback(feedback)
            
            # ìƒíƒœ ë©”ì‹œì§€ ê²°ì •
            if feedback.stable and feedback.floating_bricks == 0:
                short_status = "âœ… ì•ˆì •"
            elif feedback.stable and feedback.floating_bricks > 0:
                short_status = "âš ï¸ ë¶€ë¶„ ì•ˆì • (ê³µì¤‘ë¶€ì–‘ ì¡´ì¬)"
            else:
                short_status = "âŒ ë¶ˆì•ˆì •"
            
            print(f"  ê²°ê³¼: {short_status}")
            
            # ë¶ˆì•ˆì •í•˜ê±°ë‚˜ ë¶€ë¶„ ì•ˆì •ì´ë©´ ìƒì„¸ ë‚´ìš© ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            if not feedback.stable or feedback.floating_bricks > 0:
                 summary_text = feedback_text.replace('\n', ', ').replace('\r', '')
                 if len(summary_text) > 200:
                     summary_text = summary_text[:200] + "..."
                 print(f"  ìš”ì•½: {summary_text}")
            
            # ê³µì¤‘ë¶€ì–‘ ë¸Œë¦­ ID ìºì‹± (Toolì—ì„œ ì‚¬ìš©)
            floating_ids = []
            for ev in stab_result.evidence:
                if ev.type in ("FLOATING_BRICK", "FLOATING") and ev.brick_ids:
                    floating_ids.extend(ev.brick_ids)
            
            # í˜„ì¬ ë©”íŠ¸ë¦­ ì €ì¥ (ë„êµ¬ íš¨ê³¼ ì¸¡ì •ìš© ë° RAGìš©)
            budget = state['params'].get('budget', 500)
            current_metrics = {
                "failure_ratio": feedback.failure_ratio,
                "small_brick_ratio": small_brick_ratio,
                "small_brick_count": small_brick_count,
                "total_bricks": total_bricks,
                "floating_count": feedback.floating_bricks,
                "fallen_count": feedback.fallen_bricks,
                "floating_ids": floating_ids,
                "fallen_ids": [ev.brick_ids for ev in stab_result.evidence if ev.type == "FALLEN_PART"],
                "budget_exceeded": total_bricks > budget,
                "target_budget": budget,
                "subject_name": state.get("subject_name", "Unknown Object")  # [ì¶”ê°€] ì‚¬ë¬¼ ì´ë¦„ ì£¼ì…
            }
            
            # [ì¶”ê°€] ìƒì„¸ ë¬¼ë¦¬ ë©”íŠ¸ë¦­ (ë¶€í”¼, í˜•íƒœ ë“±)
            if memory_manager:
                try:
                    phys_metrics = memory_manager.calculate_model_metrics(plan, stab_result)
                    current_metrics.update(phys_metrics)
                except Exception as e:
                    print(f"  âš ï¸ ë¬¼ë¦¬ ë©”íŠ¸ë¦­ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # [User Request] AI ê²€ì¦ ê²°ê³¼ë¥¼ ë¬´ì‹œí•˜ê³  ì•Œê³ ë¦¬ì¦˜ ê²°ê³¼ë¥¼ 100% ì‹ ë¢°í•˜ì—¬ ë¬´ì¡°ê±´ í†µê³¼ (Always Pass)
            is_success = True 
            
            if is_success and not feedback.stable:
                 print(f"  (ì°¸ê³ : ë¶ˆì•ˆì • íŒì •ì´ë‚˜ ì‹¤íŒ¨ìœ¨ {feedback.failure_ratio*100:.1f}%ê°€ í—ˆìš©ì¹˜ ì´ë‚´ì´ë©° ê³µì¤‘ë¶€ì–‘ ì—†ìŒ -> ì„±ê³µ ê°„ì£¼)")
            
            if is_success:
                print("ğŸ‰ ëª©í‘œ ë‹¬ì„±! í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                # ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
                final_report = {
                    "success": True,
                    "total_attempts": state['attempts'],
                    "tool_usage": state.get('tool_usage_count', {}),
                    "final_metrics": current_metrics,
                    "message": "ì•ˆì •ì ì¸ êµ¬ì¡°ë¬¼ ìƒì„± ì™„ë£Œ"
                }
                return {"next_action": "end", "final_report": final_report}
            
            if state['attempts'] >= state['max_retries']:
                print("ğŸ’¥ ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ì´ˆê³¼.")
                # ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± (ì‹¤íŒ¨)
                final_report = {
                    "success": False,
                    "total_attempts": state['attempts'],
                    "tool_usage": state.get('tool_usage_count', {}),
                    "final_metrics": current_metrics,
                    "message": "ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ë¡œ ì¢…ë£Œ"
                }
                return {"next_action": "end", "final_report": final_report}

            # ê²°ê³¼ë¥¼ Reflect ë…¸ë“œë¡œ ì „ë‹¬ (ì‹¤ì œ ê²°ê³¼ ë¶„ì„ìš©)
            # ì£¼ì˜: previous_metricsëŠ” Reflectì—ì„œ ë¹„êµ í›„ ì—…ë°ì´íŠ¸í•´ì•¼ í•˜ë¯€ë¡œ ì—¬ê¸°ì„  ê±´ë“œë¦¬ì§€ ì•ŠìŒ
            
            # LLMì—ê²Œ ì „ë‹¬í•  ë©”ì‹œì§€ ë³´ê°•
            custom_feedback = feedback_text
            
            if is_over_budget:
                custom_feedback += f"\n\nğŸš¨ **ì¤‘ìš”: ì˜ˆì‚° ì´ˆê³¼! í˜„ì¬ {total_bricks}ê°œ ë¸Œë¦­ì…ë‹ˆë‹¤. ëª©í‘œ ì˜ˆì‚°ì€ {budget}ê°œì…ë‹ˆë‹¤. `TuneParameters` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ `target` ê°’ì„ ì¤„ì—¬ì•¼ í•©ë‹ˆë‹¤.**"

            elif feedback.floating_bricks > 0:
                custom_feedback += "\n\nâš ï¸ **ì¤‘ìš”: ì•„ì§ ê³µì¤‘ë¶€ì–‘(Floating) ë¸Œë¦­ì´ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤. TuneParametersë¡œ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì—¬ ì•Œê³ ë¦¬ì¦˜ì´ ë” ì•ˆì •ì ì¸ êµ¬ì¡°ë¥¼ ìƒì„±í•˜ë„ë¡ í•´ì•¼ í•©ë‹ˆë‹¤.**"
            
            return {
                "verification_raw_result": stab_result,
                "floating_bricks_ids": floating_ids,
                "messages": [HumanMessage(content=custom_feedback)],
                "current_metrics": current_metrics,   # Reflectì—ì„œ ì‹¤ì œ ê²°ê³¼ ë¶„ì„ìš©
                "next_action": "reflect"  # Verify í›„ Reflectë¡œ ì´ë™
            }
            
        except Exception as e:
            print(f"  âŒ ê²€ì¦ ì¤‘ ì—ëŸ¬: {e}")
            # ê²€ì¦ ì—ëŸ¬ ì‹œ ì¬ì‹œë„
            verification_errors = state.get('verification_errors', 0) + 1
            if verification_errors >= 3:
                # 3íšŒ ì´ìƒ ì‹¤íŒ¨ ì‹œ ì¬ìƒì„±ìœ¼ë¡œ ì „í™˜
                print(f"  âš ï¸ ê²€ì¦ ì—ëŸ¬ {verification_errors}íšŒ - ì¬ìƒì„±ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                return {
                    "messages": [HumanMessage(content=f"ê²€ì¦ ì‹œìŠ¤í…œ ì—ëŸ¬ê°€ ë°˜ë³µë¨: {e}")],
                    "verification_errors": 0,
                    "next_action": "model"
                }
            else:
                # ì¬ì‹œë„
                print(f"  ğŸ”„ ê²€ì¦ ì¬ì‹œë„ ({verification_errors}/3)...")
                import time
                time.sleep(1)  # ê²€ì¦ ì•ˆì •í™” ëŒ€ê¸°
                return {"verification_errors": verification_errors, "next_action": "verifier"}

    def node_model(self, state: AgentState) -> Dict[str, Any]:
        """LLMì´ ìƒí™©ì„ ë¶„ì„í•˜ê³  ë„êµ¬ë¥¼ ì„ íƒí•˜ëŠ” ë…¸ë“œ"""
        import time
        # API Rate Limit (429) ë°©ì§€ë¥¼ ìœ„í•œ ì§§ì€ ë”œë ˆì´ (íŠ¹íˆ Free Tier ì‚¬ìš© ì‹œ)
        time.sleep(2) 
        
        print("\n[Co-Scientist] ìƒí™© ë¶„ì„ ì¤‘...")
        self._log("ANALYZE", "AIê°€ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ê³  ê°œì„  ë°©ì•ˆì„ ì°¾ê³  ìˆìŠµë‹ˆë‹¤...")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ì •ì˜
        tools = [TuneParameters]
    
        # --- [ì „ëµ ê°€ì´ë“œ ì£¼ì…] ---
        # ì•ˆì •ì„± ë“±ê¸‰ì— ë”°ë¥¸ ì „ëµ íŒíŠ¸ ì¶”ê°€
        messages_to_send = state['messages'][:]
        
        # --- [Memory ì •ë³´ ì£¼ì… (RAG)] ---
        # Vector Searchë¥¼ í†µí•´ í˜„ì¬ ìƒí™©ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ê³¼ê±° ì‚¬ë¡€ë¥¼ ê²€ìƒ‰
        
        # í˜„ì¬ ê´€ì°° + ì‚¬ë¬¼ ì´ë¦„ ê²°í•© (ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ ì¿¼ë¦¬ ê³ ë„í™”)
        last_human_msg = next((m for m in reversed(messages_to_send) if isinstance(m, HumanMessage)), None)
        subject_prefix = f"[{state.get('subject_name', 'Object')}] "
        current_observation = subject_prefix + (last_human_msg.content if last_human_msg else "")
        
        if memory_manager:
            # 1. ë„“ì€ ë²”ìœ„ ê²€ìƒ‰ (Top 10) - ë©”íŠ¸ë¦­ í¬í•¨
            verification_metrics = state.get("verification_result")
            raw_cases = memory_manager.search_similar_cases(
                current_observation, 
                limit=10, 
                min_score=0.4, # ì˜ë¯¸ë¡ ì  ê²°í•© ì‹œ ì•½ê°„ ë‚®ì¶°ì„œ ë” ë§ì€ í›„ë³´êµ° í™•ë³´
                verification_metrics=verification_metrics,
                subject_name=state.get("subject_name", "Object")
            )
            # 2. LLM Re-ranking (Top 3 ì„ ë³„)
            similar_cases = self._rerank_and_filter_cases(current_observation, raw_cases)
            
            if similar_cases:
                memory_info = "\n**ğŸ“š ìœ ì‚¬í•œ ê³¼ê±° ì‹¤í—˜ ì‚¬ë¡€ (RAG):**\n"
                for i, case in enumerate(similar_cases, 1):
                    exp = case.get('experiment', {})
                    ver = case.get('verification', {})
                    imp = case.get('improvement', {})
                    
                    # ë¬¼ë¦¬ ë©”íŠ¸ë¦­ ì¶”ì¶œ (ê³ ë„í™”ë¨)
                    metrics = ver.get('metrics_after', ver)
                    vol = metrics.get('total_volume', 0)
                    dims = metrics.get('dimensions', {})
                    dim_str = f"{dims.get('width', 0):.0f}x{dims.get('height', 0):.0f}x{dims.get('depth', 0):.0f}" if dims else "N/A"
                    
                    tool = exp.get('tool', 'Unknown')
                    result = ver.get('numerical_analysis', 'N/A')
                    lesson = imp.get('lesson_learned', 'No lesson')
                    outcome = "ì„±ê³µ" if case.get('result_success') else "ì‹¤íŒ¨"
                    score = case.get('similarity_score', 0)
                    rel = case.get('reliability_grade', 'Low') # ì ìˆ˜ì— ë”°ë¥¸ ì‹ ë¢°ë„
                    
                    memory_info += f"[{i}] {outcome} ì‚¬ë¡€ (ì‹ ë¢°ë„: {rel}, ìœ ì‚¬ë„: {score:.2f})\n"
                    memory_info += f"    - ë¬¼ë¦¬ íŠ¹ì„±: ë¶€í”¼ {vol:.1f}, í¬ê¸° {dim_str}, ë¸Œë¦­ {metrics.get('total_bricks', 0)}ê°œ\n"
                    memory_info += f"    - ë„êµ¬: {tool} -> ê²°ê³¼: {result}\n"
                    memory_info += f"    - êµí›ˆ: {lesson}\n"
                
                memory_info += "\nìœ„ ë¶€í”¼ì™€ í˜•íƒœì  ìœ ì‚¬ì„±ì„ ê³ ë ¤í•˜ì—¬ ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê²°ì •í•˜ì„¸ìš”.\n"
                messages_to_send.append(SystemMessage(content=memory_info))
                print(f"  ğŸ“š RAG ê²€ìƒ‰ ê²°ê³¼ {len(similar_cases)}ê±´ ì£¼ì…ë¨")
        
        # Legacy Memory (Fallback)
        memory = state.get('memory', {})
        lessons = memory.get('lessons', [])
        failed_approaches = memory.get('failed_approaches', [])
        
        if lessons or failed_approaches:
            memory_info = "\n**ğŸ“š ì´ì „ ê²½í—˜ (Memory):**\n"
            if lessons:
                memory_info += "- ìµœê·¼ êµí›ˆ: " + "; ".join(lessons[-3:]) + "\n"
            if failed_approaches:
                memory_info += "- í”¼í•´ì•¼ í•  ì ‘ê·¼ë²•: " + "; ".join(failed_approaches[-3:]) + "\n"
            
            memory_msg = SystemMessage(content=memory_info)
            messages_to_send.append(memory_msg)
            print(f"  ğŸ“š Memory ì •ë³´ {len(lessons)}ê°œ êµí›ˆ ì „ë‹¬ë¨")
        
        # ì§ì „ ê²€ì¦ ê²°ê³¼ í™•ì¸ - ì•ˆì •ì„± ë“±ê¸‰ ê¸°ë°˜ ì „ëµ íŒíŠ¸
        last_msg = messages_to_send[-1]

        if isinstance(last_msg, HumanMessage) and "ê²€ì¦ ê²°ê³¼" in str(last_msg.content):
            content = str(last_msg.content)

            # ì•ˆì •ì„± ë“±ê¸‰ íŒŒì‹±
            import re
            grade_match = re.search(r"ì•ˆì •ì„± ë“±ê¸‰: \S+ \((\w+)\)", content)
            score_match = re.search(r"ì ìˆ˜: (\d+)/100", content)

            grade = grade_match.group(1) if grade_match else "UNKNOWN"
            score = int(score_match.group(1)) if score_match else 0

            hint = ""
            if grade == "UNSTABLE":
                print(f"  ğŸ’¡ [Strategy Hint] ë¶ˆì•ˆì • (ì ìˆ˜: {score}) -> íŒŒë¼ë¯¸í„° ëŒ€í­ ë³€ê²½ í•„ìš”")
                hint = f"""**ğŸ”´ ë¶ˆì•ˆì • (UNSTABLE, {score}ì )**
êµ¬ì¡°ê°€ ë¬´ë„ˆì¡ŒìŠµë‹ˆë‹¤. target, budget, interlock, fill ë“± í•µì‹¬ íŒŒë¼ë¯¸í„°ë¥¼ ëŒ€í­ ë³€ê²½í•˜ì—¬ ì¬ìƒì„±í•˜ì„¸ìš”.
íŠ¹íˆ interlock=True, fill=True, support_ratioë¥¼ ë†’ì´ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”."""
            elif grade == "MEDIUM":
                print(f"  ğŸ’¡ [Strategy Hint] ì¤‘ê°„ (ì ìˆ˜: {score}) -> íŒŒë¼ë¯¸í„° ì†Œí­ ì¡°ì • í•„ìš”")
                hint = f"""**ğŸŸ¡ ì¤‘ê°„ (MEDIUM, {score}ì )**
êµ¬ì¡°ê°€ ê¸°ìš¸ì–´ì§‘ë‹ˆë‹¤. support_ratio, interlock, fill ë“±ì„ ë¯¸ì„¸ ì¡°ì •í•˜ì—¬ ì•ˆì •ì„±ì„ ë†’ì´ì„¸ìš”.
í° ë³€ê²½ë³´ë‹¤ëŠ” ì†Œí­ ì¡°ì •ì´ íš¨ê³¼ì ì…ë‹ˆë‹¤."""

            if hint:
                messages_to_send.append(SystemMessage(content=hint))

        # ëª¨ë¸ ë°”ì¸ë”© ë° í˜¸ì¶œ
        try:
            # [Rollback] ë¬´ì¡°ê±´ Gemini ì‚¬ìš©
            client_to_use = self.gemini_client
            print(f"  ğŸ¤– Active Model: Gemini-2.5-Flash (Fixed)")
            
            model_with_tools = client_to_use.bind_tools(tools)
            response = model_with_tools.invoke(messages_to_send)
            
            # ì‘ë‹µ í™•ì¸
            if response.tool_calls:
                print(f"  ğŸ”¨ ë„êµ¬ ì„ íƒ: {[tc['name'] for tc in response.tool_calls]}")
                return {"messages": [response], "next_action": "tool"}
            else:
                # ë„êµ¬ë¥¼ ì„ íƒí•˜ì§€ ì•Šì€ ê²½ìš° (ëë‚¬ë‹¤ê³  íŒë‹¨)
                print(f"  ğŸ’­ LLM ì˜ê²¬: {response.content}")
                
                # ì‹¤ì œ ì„±ê³µ ì—¬ë¶€ ì¬í™•ì¸
                current_metrics = state.get('current_metrics', {})
                floating_count = current_metrics.get('floating_count', 0)
                failure_ratio = current_metrics.get('failure_ratio', 0)
                
                # ê³µì¤‘ë¶€ì–‘ì´ ì—†ê³  ì‹¤íŒ¨ìœ¨ì´ ë‚®ìœ¼ë©´ ì¢…ë£Œ í—ˆìš©
                if floating_count == 0 and failure_ratio <= state['acceptable_failure_ratio']:
                    print("ğŸ‰ ëª¨ë“  ì¡°ê±´ ì¶©ì¡±. ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    return {"messages": [response], "next_action": "end"}
                else:
                    # ì•„ì§ ë¬¸ì œê°€ ë‚¨ì•˜ëŠ”ë° ì¢…ë£Œí•˜ë ¤ê³  í•˜ë©´ íŒíŠ¸ë¥¼ ì£¼ê³  ì¬ì‹œë„
                    print(f"âš ï¸ ê²½ê³ : ë¬¸ì œê°€ ë‚¨ì•˜ëŠ”ë°({floating_count}ê°œ ê³µì¤‘ë¶€ì–‘) ì¢…ë£Œ ì‹œë„í•¨. ì¬ì§€ì‹œ ì¤‘...")
                    error_feedback = f"ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. {floating_count}ê°œì˜ ê³µì¤‘ë¶€ì–‘ ë¸Œë¦­ì´ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤. TuneParametersë¡œ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì—¬ ì•Œê³ ë¦¬ì¦˜ì´ ë” ì•ˆì •ì ì¸ êµ¬ì¡°ë¥¼ ìƒì„±í•˜ë„ë¡ í•˜ì„¸ìš”."
                    hint = HumanMessage(content=error_feedback)
                    return {"messages": [response, hint], "next_action": "model"}
                
        except Exception as e:
            print(f"  âš ï¸ LLM í˜¸ì¶œ ì—ëŸ¬: {e}")
            if "429" in str(e):
                print("  ğŸ’¤ API í• ë‹¹ëŸ‰ ì´ˆê³¼. ì ì‹œ ëŒ€ê¸° í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                time.sleep(10)
                return {"next_action": "model"}
            return {"next_action": "end"}

    def node_tool_executor(self, state: AgentState) -> Dict[str, Any]:
        """ì„ íƒëœ ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ëŠ” ë…¸ë“œ"""
        last_message = state['messages'][-1]
        
        if not isinstance(last_message, AIMessage) or not last_message.tool_calls:
            return {"next_action": "model"}
        
        tool_results = []
        next_step = "model" # ê¸°ë³¸ê°’
        
        # ë„êµ¬ ì‚¬ìš© ì¶”ì  ì´ˆê¸°í™”
        tool_usage_count = state.get('tool_usage_count', {})
        last_tool_used = state.get('last_tool_used', None)
        consecutive_same_tool = state.get('consecutive_same_tool', 0)
        previous_metrics = state.get('previous_metrics', {})
        
        for tool_call in last_message.tool_calls:
            tool_name = tool_call['name']
            args = tool_call['args']
            tool_call_id = tool_call['id']
            
            # ë¬´í•œ ë£¨í”„ ë°©ì§€: ê°™ì€ ë„êµ¬ ì—°ì† ì‚¬ìš© ì²´í¬
            if tool_name == last_tool_used:
                consecutive_same_tool += 1
            else:
                consecutive_same_tool = 1
            
            # 3íšŒ ì—°ì† ê°™ì€ ë„êµ¬ ì‚¬ìš© ì‹œ ê²½ê³ 
            if consecutive_same_tool >= 3:
                print(f"  âš ï¸ ê²½ê³ : {tool_name}ì„(ë¥¼) {consecutive_same_tool}íšŒ ì—°ì† ì‚¬ìš© ì¤‘!")
                warning_msg = f"'{tool_name}'ì„(ë¥¼) 3íšŒ ì—°ì† ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì „ëµì„ ê³ ë ¤í•´ì£¼ì„¸ìš”."
                tool_results.append(ToolMessage(
                    content=warning_msg,
                    tool_call_id=tool_call_id
                ))
                return {
                    "messages": tool_results,
                    "next_action": "model",
                    "tool_usage_count": tool_usage_count,
                    "last_tool_used": tool_name,
                    "consecutive_same_tool": consecutive_same_tool,
                }
            
            # ë„êµ¬ ì‚¬ìš© íšŸìˆ˜ ì—…ë°ì´íŠ¸
            tool_usage_count[tool_name] = tool_usage_count.get(tool_name, 0) + 1
            
            print(f"\n[Tool Execution] {tool_name} ì‹¤í–‰... (ì´ {tool_usage_count[tool_name]}íšŒ)")
            
            result_content = ""
            
            if tool_name == "TuneParameters":
                # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
                new_params = state['params'].copy()
                new_params.update(args)
                # shrinkëŠ” ë‚´ë¶€ ìµœì í™” íŒŒë¼ë¯¸í„°ì´ë¯€ë¡œ ê³ ì •ê°’ ì‚¬ìš© (LLMì´ ì¡°ì • ë¶ˆê°€)
                new_params['shrink'] = 0.7
                result_content = f"íŒŒë¼ë¯¸í„°ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤. ({args})"
                
                # íŒŒë¼ë¯¸í„°ê°€ ë°”ë€Œì—ˆìœ¼ë‹ˆ ì¬ìƒì„±(Generator)ìœ¼ë¡œ ì´ë™
                next_step = "generator"
                
                # ì—…ë°ì´íŠ¸ëœ íŒŒë¼ë¯¸í„° ë°˜í™˜
                state['params'] = new_params
                
            else:
                result_content = f"ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬: {tool_name}"
            
            print(f"  ê²°ê³¼: {result_content}")
            
            tool_results.append(ToolMessage(
                content=result_content,
                tool_call_id=tool_call_id
            ))
            
        # ToolMessageë“¤ì„ Historyì— ì¶”ê°€í•˜ê³ , ë‹¤ìŒ ë‹¨ê³„ë¡œ ì´ë™
        # verifierë¡œ ë°”ë¡œ ì´ë™ (Verify í›„ Reflectì—ì„œ ì‹¤ì œ ê²°ê³¼ ë¶„ì„)
        
        return {
            "messages": tool_results, 
            "next_action": next_step,  # verifier ë˜ëŠ” generatorë¡œ ì§ì ‘ ì´ë™
            "params": state['params'],
            "tool_usage_count": tool_usage_count,
            "last_tool_used": tool_name,
            "consecutive_same_tool": consecutive_same_tool,
        }

    def node_reflect(self, state: AgentState) -> Dict[str, Any]:
        """
        íšŒê³  ë…¸ë“œ: ê²€ì¦ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ì„±ê³µ/ì‹¤íŒ¨ë¥¼ Memoryì— ê¸°ë¡í•©ë‹ˆë‹¤.
        Co-Scientistì˜ í•µì‹¬ í•™ìŠµ ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤.
        
        ì´ì œ Verify í›„ì— í˜¸ì¶œë˜ë¯€ë¡œ ì‹¤ì œ ê²°ê³¼ë¥¼ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
        print("\n[Reflect] ì‹¤ì œ ê²°ê³¼ ë¶„ì„ ì¤‘...")
        self._log("REFLECT", "ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  í•™ìŠµí•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        # Memory ì´ˆê¸°í™” (ì—†ìœ¼ë©´)
        memory = state.get('memory', {
            "failed_approaches": [],
            "successful_patterns": [],
            "lessons": [],
            "consecutive_failures": 0
        })
        
        previous_metrics = state.get('previous_metrics', {})
        current_metrics = state.get('current_metrics', {})
        last_tool = state.get('last_tool_used', 'unknown')
        
        # ì´ì „ ë©”íŠ¸ë¦­ì´ ì—†ìœ¼ë©´ ì²« ì‹¤í–‰
        # ì´ì „ ë©”íŠ¸ë¦­ì´ ì—†ìœ¼ë©´ ì²« ì‹¤í–‰ (ë¹„êµ ëŒ€ìƒ ì—†ìŒ)
        if not previous_metrics:
            print("  (ì²« ê²€ì¦ - ê¸°ì¤€ì  ì„¤ì •)")
            return {
                "memory": memory, 
                "previous_metrics": current_metrics, # ê¸°ì¤€ì  ì„¤ì •
                "next_action": "hypothesize"
            }
        
        # ë©”íŠ¸ë¦­ ë¹„êµ
        prev_failure = previous_metrics.get('failure_ratio', 0)
        curr_failure = current_metrics.get('failure_ratio', 0)
        prev_floating = previous_metrics.get('floating_count', 0)
        curr_floating = current_metrics.get('floating_count', 0)
        prev_small_ratio = previous_metrics.get('small_brick_ratio', 0)
        curr_small_ratio = current_metrics.get('small_brick_ratio', 0)
        
        # íš¨ê³¼ íŒì •
        failure_improved = curr_failure < prev_failure
        floating_improved = curr_floating < prev_floating
        small_ratio_improved = curr_small_ratio < prev_small_ratio
        
        overall_improved = failure_improved or floating_improved
        
        # ë„êµ¬ë³„ ê²°ê³¼ ë¶„ì„ ë° ê¸°ë¡
        # 2. ê²°ê³¼ ë¶„ì„ ë° í†µí•© ë¡œê·¸ ì €ì¥ (Unified Log)
        if memory_manager:
            try:
                # ê´€ì°° (Observation)
                observation = f"ratio={prev_small_ratio:.2f}, floating={prev_floating}, failure={prev_failure:.2f}"
                
                current_hypothesis = state.get('current_hypothesis', {})
                hyp_text = current_hypothesis.get('hypothesis', 'No hypothesis')

                # ê°„ë‹¨í•œ ì„±ê³µ/ì‹¤íŒ¨ íŒì • ë° ë©”ì‹œì§€
                if overall_improved:
                    lesson = f"âœ… {last_tool} ì„±ê³µ: {hyp_text} (Gained Improvement)"
                    memory["successful_patterns"].append(f"{last_tool}: íš¨ê³¼ ìˆìŒ")
                    memory["consecutive_failures"] = 0
                    print(f"  {lesson}")
                else:
                    lesson = f"âŒ {last_tool} ì‹¤íŒ¨: {hyp_text} (No Improvement)"
                    memory["failed_approaches"].append(f"{last_tool}: íš¨ê³¼ ë¯¸ë¯¸")
                    memory["consecutive_failures"] += 1
                    print(f"  {lesson}")
                
                memory["lessons"].append(lesson)
                
                # ë¦¬ìŠ¤íŠ¸ ê´€ë¦¬
                memory["lessons"] = memory["lessons"][-10:]
                memory["failed_approaches"] = memory["failed_approaches"][-5:]
                memory["successful_patterns"] = memory["successful_patterns"][-5:]

                # DB & Vector Store ì €ì¥ (í‘œì¤€í™”ëœ í—¬í¼ í•¨ìˆ˜ ì‚¬ìš©)
                memory_manager.log_experiment(
                    session_id=state.get('session_id', 'unknown_session'),
                    model_id=Path(state['glb_path'] or state['ldr_path']).name,
                    agent_type="main_agent",
                    iteration=state['attempts'],
                    hypothesis=build_hypothesis(
                        observation=observation,
                        hypothesis=hyp_text,
                        reasoning=f"Based on memory lessons: {memory.get('lessons', [])[-1] if memory.get('lessons') else 'None'}",
                        prediction=f"floating: {prev_floating}â†’{curr_floating}, ratio: {prev_small_ratio:.2f}â†’?"
                    ) if build_hypothesis else {"observation": observation},
                    experiment=build_experiment(
                        tool=last_tool,
                        parameters=state.get('params', {}),
                        model_name="gemini-2.5-flash"
                    ) if build_experiment else {"tool": last_tool},
                    verification=build_verification(
                        passed=overall_improved,
                        metrics_before=previous_metrics,
                        metrics_after=current_metrics,
                        numerical_analysis=f"floating {prev_floating}â†’{curr_floating}, ratio {prev_small_ratio:.2f}â†’{curr_small_ratio:.2f}, failure {prev_failure:.2f}â†’{curr_failure:.2f}"
                    ) if build_verification else {"passed": overall_improved},
                    improvement=build_improvement(
                        lesson_learned=lesson,
                        next_hypothesis="Maintain strategy" if overall_improved else "Change strategy"
                    ) if build_improvement else {"lesson_learned": lesson}
                )
            except Exception as e:
                print(f"âš ï¸ [Memory] í†µí•© ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        return {
            "memory": memory, 
            "previous_metrics": current_metrics, # ë‹¤ìŒ í„´ì„ ìœ„í•´ í˜„ì¬ ë©”íŠ¸ë¦­ ìŠ¹ê²©
            "next_action": "hypothesize"
        }


    # --- Build Graph ---

    def build(self):
        workflow = StateGraph(AgentState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("generator", self.node_generator)
        workflow.add_node("verifier", self.node_verifier)
        workflow.add_node("model", self.node_model)
        workflow.add_node("tool_executor", self.node_tool_executor)
        workflow.add_node("reflect", self.node_reflect)      # íšŒê³  (í•™ìŠµ)
        workflow.add_node("hypothesize", self.node_hypothesize)  # [v2] ê°€ì„¤ ìƒì„±
        workflow.add_node("strategy", self.node_strategy)        # [v2] ì „ëµ ê²°ì •
        
        # ë¼ìš°íŒ… ë¡œì§
        def route_next(state: AgentState):
            return state['next_action']
            
        # ì—£ì§€ ì •ì˜
        # 1. Generator -> Verify
        workflow.add_conditional_edges("generator", route_next, {"verify": "verifier", "model": "model"})
        
        # 2. Verifier -> Reflect (ì„±ê³µ ì‹œ End)
        workflow.add_conditional_edges("verifier", route_next, {
            "model": "model",       # ì—ëŸ¬ ë“± ì˜ˆì™¸ ì‹œ
            "end": END,             # ì„±ê³µ ë˜ëŠ” í¬ê¸° ì‹œ
            "verifier": "verifier", # ì¬ì‹œë„ ì‹œ
            "reflect": "reflect"    # ê²€ì¦ ì™„ë£Œ í›„ íšŒê³ ë¡œ ì´ë™
        })
        
        # 3. Reflect -> Hypothesize (v2 í•µì‹¬: íšŒê³  í›„ ë°”ë¡œ ëª¨ë¸ì´ ì•„ë‹ˆë¼ ê°€ì„¤ ìˆ˜ë¦½ìœ¼ë¡œ)
        # ë‹¨, ì²« ì‹¤í–‰ì´ë¼ ë¹„êµí•  ê²Œ ì—†ìœ¼ë©´ ë°”ë¡œ Strategyë‚˜ Modelë¡œ ê°ˆ ìˆ˜ë„ ìˆìŒ
        workflow.add_conditional_edges("reflect", route_next, {
            "model": "model",             # ë°”ë¡œ ëª¨ë¸ë¡œ ê°€ëŠ” ê²½ìš° (Legacy)
            "hypothesize": "hypothesize"  # ë³´í†µ ê°€ì„¤ ìƒì„±ìœ¼ë¡œ ì´ë™
        })
        
        # 4. Hypothesize -> Strategy
        workflow.add_conditional_edges("hypothesize", route_next, {"strategy": "strategy"})
        
        # 5. Strategy -> Model (ëª¨ë¸ ì„¤ì • í›„ ë„êµ¬ ì„ íƒ)
        workflow.add_conditional_edges("strategy", route_next, {"model": "model"})
        
        # 6. Model -> Tool
        workflow.add_conditional_edges("model", route_next, {"tool": "tool_executor", "model": "model", "end": END})
        
        # 7. Tool -> Generator or Verifier
        workflow.add_conditional_edges("tool_executor", route_next, {
            "generator": "generator", 
            "verifier": "verifier", 
            "model": "model",
        })
        
        workflow.set_entry_point("generator")
        
        return workflow.compile()


# ============================================================================
# Evolver Post-Processing (ì„œë¸Œí”„ë¡œì„¸ìŠ¤ ë°©ì‹)
# ============================================================================

def _run_evolver_subprocess(ldr_path: str, glb_path: str = None) -> dict:
    """Evolver ì—ì´ì „íŠ¸ë¥¼ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰ (í˜•íƒœ ê°œì„ )

    ë©”ì¸ ì—ì´ì „íŠ¸ ì™„ë£Œ í›„ í›„ì²˜ë¦¬ë¡œ ì‹¤í–‰.
    agent íŒ¨í‚¤ì§€ ì´ë¦„ ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ ë³„ë„ í”„ë¡œì„¸ìŠ¤ë¡œ ê²©ë¦¬.
    ì‹¤íŒ¨í•´ë„ ì›ë³¸ LDRì€ ë³´ì¡´ë¨.
    """
    import subprocess
    import shutil

    evolver_script = _BRICK_ENGINE_DIR / "exporter" / "evolver" / "run_agent.py"

    if not evolver_script.exists():
        return {"success": False, "reason": "evolver run_agent.py not found"}

    if not Path(ldr_path).exists():
        return {"success": False, "reason": "LDR file not found"}

    cmd = [sys.executable, str(evolver_script), str(ldr_path)]
    if glb_path and Path(glb_path).exists():
        cmd.append(str(glb_path))

    try:
        result = subprocess.run(
            cmd,
            timeout=300,
            cwd=str(evolver_script.parent),
        )

        ldr_p = Path(ldr_path)
        evolved_path = ldr_p.parent / f"{ldr_p.stem}_evolved.ldr"

        if evolved_path.exists() and evolved_path.stat().st_size > 0:
            shutil.copy2(str(evolved_path), str(ldr_path))
            evolved_path.unlink()
            return {"success": True}
        else:
            return {"success": False, "reason": "No evolved file generated"}

    except subprocess.TimeoutExpired:
        return {"success": False, "reason": "Timeout (5min)"}
    except Exception as e:
        return {"success": False, "reason": str(e)}


# ============================================================================
# ì‹¤í–‰ í•¨ìˆ˜
# ============================================================================

def regeneration_loop(
    glb_path: str,
    output_ldr_path: str,
    subject_name: str = "Unknown Object", # [ì¶”ê°€] ì‚¬ë¬¼ì˜ ì´ë¦„
    llm_client: Optional[BaseLLMClient] = None,
    max_retries: int = 5,
    acceptable_failure_ratio: float = 0.1,
    gui: bool = False,
    params: Optional[Dict[str, Any]] = None,
    log_callback = None,
    user_email: str = "unknown"
):
    print("=" * 60)
    print("ğŸ¤– Co-Scientist Agent (Tool-Use Ver.)")
    print("=" * 60)

    # ë¡œê·¸ ì½œë°± ì¶”ì¶œ (kids_render.pyì—ì„œ ì£¼ì…)
    log_callback = log_callback or (params.pop("log_callback", None) if params else None)
    def _log(step, msg):
        if log_callback:
            try:
                log_callback(step, msg)
            except Exception:
                pass  # fire-and-forget

    _log("ANALYZE", "ëª¨ë¸ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")

    graph_builder = RegenerationGraph(llm_client, log_callback=log_callback)
    graph_builder.user_email = user_email
    app = graph_builder.build()
    
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ë° ì´ˆê¸° ì„¤ì •
    system_msg = SystemMessage(content=graph_builder.SYSTEM_PROMPT)
    
    # DBì—ì„œ Memory ë¡œë“œ
    initial_memory = {
        "failed_approaches": [],
        "successful_patterns": [],
        "lessons": [],
        "consecutive_failures": 0
    }
    try:
        model_id = Path(glb_path).name
        loaded_mem = load_memory_from_db(model_id)
        if loaded_mem:
            initial_memory.update(loaded_mem)
    except Exception as e:
        print(f"âš ï¸ [Memory] ì´ˆê¸° ë¡œë“œ ì‹¤íŒ¨: {e}")

    # íŒŒë¼ë¯¸í„° ë³‘í•© (ê¸°ë³¸ê°’ + ì‚¬ìš©ì ì…ë ¥)
    merged_params = DEFAULT_PARAMS.copy()
    if params:
        merged_params.update(params)
        print(f"âš™ï¸  Custom Params Applied: {list(params.keys())}")

    initial_state = AgentState(
        glb_path=glb_path,
        ldr_path=output_ldr_path,
        subject_name=subject_name,      # [ì¶”ê°€] ì‚¬ë¬¼ ì´ë¦„ ì£¼ì…
        params=merged_params,
        attempts=0,
        session_id=memory_manager.start_session(Path(glb_path).name, "main_agent") if memory_manager else "offline",
        max_retries=max_retries,
        acceptable_failure_ratio=acceptable_failure_ratio,
        verification_duration=2.0,
        gui=gui,
        messages=[
            system_msg,
            HumanMessage(content=f"'{subject_name}' ëª¨ë¸ì˜ ë¬¼ë¦¬ì  ì•ˆì •ì„±ì„ ìµœì í™”í•˜ê³  LDR íŒŒì¼ì„ ì„¤ê³„í•˜ì„¸ìš”.")
        ], # History ì‹œì‘
        verification_raw_result=None,
        floating_bricks_ids=[],
        verification_errors=0,  # ê²€ì¦ ì—ëŸ¬ ì¹´ìš´í„° ì´ˆê¸°í™”
        # ìƒˆë¡œ ì¶”ê°€ëœ í•„ë“œë“¤
        tool_usage_count={},      # ë„êµ¬ ì‚¬ìš© íšŸìˆ˜ ì¶”ì 
        last_tool_used=None,      # ë§ˆì§€ë§‰ ì‚¬ìš© ë„êµ¬
        consecutive_same_tool=0,  # ê°™ì€ ë„êµ¬ ì—°ì† ì‚¬ìš© íšŸìˆ˜
        previous_metrics={},      # ì´ì „ ë©”íŠ¸ë¦­ (íš¨ê³¼ ì¸¡ì •ìš©)
        current_metrics={},       # í˜„ì¬ ë©”íŠ¸ë¦­ (Reflectì—ì„œ ë¹„êµìš©)
        final_report={},          # ìµœì¢… ê²°ê³¼ ë¦¬í¬íŠ¸
        # Co-Scientist Memory ì´ˆê¸°í™” (DB ë¡œë“œ ë°˜ì˜)
        memory=initial_memory,
        next_action="generate",
        user_email=user_email
    )
    
    # ì‹¤í–‰
    _log("GENERATE", f"CoScientist ì—ì´ì „íŠ¸ê°€ ë¸Œë¦­ ë°°ì¹˜ë¥¼ ìµœì í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤... (GLB: {Path(glb_path).stem})")
    final_state = app.invoke(initial_state)

    _log("VERIFY", "ë¬¼ë¦¬ ì•ˆì •ì„±ì„ ê²€ì¦í•˜ê³  ìˆìŠµë‹ˆë‹¤...")

    # ============================================================
    # Post-processing: Evolver Agent (í˜•íƒœ ê°œì„ )
    # ============================================================
    if Path(output_ldr_path).exists():
        file_size = Path(output_ldr_path).stat().st_size
        print(f"[DEBUG] LDR File exists before Evolver: {output_ldr_path} (Size: {file_size} bytes)")
        
        _log("EVOLVE", "í˜•íƒœ ê°œì„  ì—ì´ì „íŠ¸ê°€ ëª¨ë¸ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        print("\n[Evolver] í˜•íƒœ ê°œì„  ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘...")
        evolver_result = _run_evolver_subprocess(output_ldr_path, glb_path)
        if evolver_result.get("success"):
            print("[Evolver] âœ… í˜•íƒœ ê°œì„  ì™„ë£Œ")
        else:
            reason = evolver_result.get("reason", "unknown")
            print(f"[Evolver] âš ï¸ í˜•íƒœ ê°œì„  ìŠ¤í‚µ: {reason}")
    else:
        print(f"[DEBUG] âŒ LDR File MISSING before Evolver: {output_ldr_path}")


    _log("REFLECT", "ëª¨ë¸ ê²€ì¦ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. ìµœì¢… ëª¨ë¸ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")

    print("\n" + "=" * 60)
    print("ğŸ“‹ ìµœì¢… ê²°ê³¼ ë¦¬í¬íŠ¸")
    print("=" * 60)
    
    # ìµœì¢… ë¦¬í¬íŠ¸ ì¶œë ¥
    report = final_state.get('final_report', {})
    if report:
        success = report.get('success', False)
        status = "âœ… ì„±ê³µ" if success else "âŒ ì‹¤íŒ¨"
        print(f"ìƒíƒœ: {status}")
        print(f"ì´ ì‹œë„: {report.get('total_attempts', final_state['attempts'])}íšŒ")
        
        # ë„êµ¬ ì‚¬ìš© í†µê³„
        tool_usage = report.get('tool_usage', {})
        if tool_usage:
            print(f"ë„êµ¬ ì‚¬ìš© í˜„í™©:")
            for tool, count in tool_usage.items():
                print(f"  - {tool}: {count}íšŒ")
        
        # ìµœì¢… ë©”íŠ¸ë¦­
        metrics = report.get('final_metrics', {})
        if metrics:
            print(f"ìµœì¢… ë©”íŠ¸ë¦­:")
            print(f"  - ì‹¤íŒ¨ìœ¨: {metrics.get('failure_ratio', 0) * 100:.1f}%")
            print(f"  - 1x1 ë¹„ìœ¨: {metrics.get('small_brick_ratio', 0) * 100:.1f}%")
            print(f"  - ì´ ë¸Œë¦­: {metrics.get('total_bricks', 0)}ê°œ")
        
        print(f"ë©”ì‹œì§€: {report.get('message', '')}")
    else:
        print(f"ì´ ì‹œë„: {final_state['attempts']}íšŒ")
    
    print("=" * 60)
    
    # ğŸ“Š ì„¸ì…˜ í”¼ë“œë°± ë³´ê³ ì„œ ìƒì„±
    if memory_manager:
        try:
            session_id = final_state.get('session_id', '')
            if session_id and session_id != 'offline':
                feedback_report = memory_manager.generate_session_report(session_id)
                if 'error' not in feedback_report:
                    print("\nğŸ“Š [Co-Scientist] ì„¸ì…˜ í”¼ë“œë°± ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
                    print(f"   - ì´ ë°˜ë³µ: {feedback_report.get('statistics', {}).get('total_iterations', 0)}íšŒ")
                    print(f"   - ì„±ê³µë¥ : {feedback_report.get('statistics', {}).get('success_rate', 0)}%")
                    print(f"   - ê¶Œì¥ì‚¬í•­: {feedback_report.get('final_recommendation', '')}")
        except Exception as e:
            print(f"âš ï¸ [Co-Scientist] ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")

    _log("COMPLETE", "ëª¨ë¸ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    return final_state


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("glb", help="ì…ë ¥ GLB íŒŒì¼")
    parser.add_argument("--out", default="output.ldr", help="ì¶œë ¥ LDR")
    parser.add_argument("--max-retries", type=int, default=5)
    parser.add_argument("--gui", action="store_true")
    parser.add_argument("--api-key", help="API Key")
    
    args = parser.parse_args()
    
    # íˆ´ ë°”ì¸ë”©ì„ ìœ„í•´ GeminiClient ì‚¬ìš© (LangChain í˜¸í™˜)
    client = GeminiClient(api_key=args.api_key)
    
    regeneration_loop(
        args.glb,
        args.out,
        llm_client=client,
        max_retries=args.max_retries,
        gui=args.gui
    )
