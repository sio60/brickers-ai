"""
Admin AI Analyst â€” Node êµ¬í˜„
7ê°œ ë…¸ë“œ: Miner â†’ Evaluator â†’ Diagnoser â†’ Strategist â†’ DeepInvestigator â†’ ReporterGreen â†’ Finalizer
"""
from __future__ import annotations

import json
import logging
from datetime import datetime
from typing import List, Dict, Any

from .state import AdminAnalystState
from .prompts import (
    DIAGNOSER_PROMPT,
    STRATEGIST_PROMPT,
    GUARDIAN_PROMPT,
    REPORTER_GREEN_PROMPT,
    FINALIZER_PROMPT,
    QUERY_ANALYST_PROMPT,
)

# ... (ê¸°ì¡´ ì½”ë“œ: miner_node, evaluator_node ìœ ì§€)


from .llm_utils import call_llm_json

log = logging.getLogger("admin_analyst.nodes")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Node 1: Miner â€” ë°ì´í„° ìˆ˜ì§‘
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def miner_node(state: AdminAnalystState) -> dict:
    """GA4 Data API + Direct MongoDBì—ì„œ ì›í•© ì§€í‘œ ë° ë¡œìš° ë°ì´í„° ìˆ˜ì§‘."""
    from service import backend_client
    from db import get_db
    import config

    log.info("[Miner] í†µí•© ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (Analytics + DB)...")

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚  PART 1: Macro Analytics (GA4 & Backend Stats)              â”‚
    # â”‚  - ì „ì²´ ì„œë¹„ìŠ¤ì˜ ê±°ì‹œì  íë¦„(íŠ¸ë˜í”½, ìœ ì…) íŒŒì•…                 â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    import asyncio

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚  PART 1: Macro Analytics (GA4 & Backend Stats)              â”‚
    # â”‚  - ë³‘ë ¬(Parallel) ì²˜ë¦¬ë¡œ ì†ë„ 10ë°° í–¥ìƒ: ëª¨ë“  API ë™ì‹œ í˜¸ì¶œ      â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    results = await asyncio.gather(
        backend_client.get_analytics_summary(7),
        backend_client.get_daily_users(14),
        backend_client.get_top_tags(7, limit=15),
        backend_client.get_heavy_users(7, limit=10),
        backend_client.get_top_posts(7, limit=5),
        backend_client.get_event_stats("generate_fail", 7),
        backend_client.get_event_stats("generate_success", 7),
        backend_client.get_event_stats("generate_success", 1),
        backend_client.get_event_stats("generate_fail", 1),
        backend_client.get_event_stats("gallery_register_attempt", 1),
        return_exceptions=True
    )

    # ê²°ê³¼ ë§¤í•‘ (ì—ëŸ¬ ë°œìƒ ì‹œ None/ë¹ˆê°’ ì²˜ë¦¬)
    summary = results[0] if not isinstance(results[0], Exception) else {}
    daily = results[1] if not isinstance(results[1], Exception) else []
    tags = results[2] if not isinstance(results[2], Exception) else []
    users = results[3] if not isinstance(results[3], Exception) else []
    top_posts = results[4] if not isinstance(results[4], Exception) else []
    fail_7d = results[5] if not isinstance(results[5], Exception) else []
    success_7d = results[6] if not isinstance(results[6], Exception) else []
    today_gen_success = results[7] if not isinstance(results[7], Exception) else []
    today_gen_fail = results[8] if not isinstance(results[8], Exception) else []
    today_gallery = results[9] if not isinstance(results[9], Exception) else []

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚  PART 2: Micro Logs (Direct MongoDB Access)                 â”‚
    # â”‚  - ê°œë³„ ì‘ì—…ì˜ êµ¬ì²´ì  ìƒíƒœ, í’ˆì§ˆ, ì—ëŸ¬ ë“± ë¯¸ì‹œì  ë°ì´í„° íŒŒì•…      â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    db_raw = {}
    try:
        db = get_db()
        # ìµœê·¼ 24ì‹œê°„ ë‚´ ìƒì„±ëœ ì‘ì—…ë“¤ì˜ ì›ë³¸ ìƒíƒœ ìš”ì•½
        one_day_ago = datetime.now().timestamp() - 86400
        jobs_col = db["kids_jobs"]
        
        # ì„±ê³µí–ˆê±°ë‚˜ ì‹¤íŒ¨í•œ ì‘ì—… ëª¨ë‘ í¬í•¨í•˜ì—¬ ë¶„ì„ (ìµœëŒ€ 200ê±´ ìƒ˜í”Œë§)
        recent_jobs = list(jobs_col.find({
            "createdAt": {"$gte": datetime.fromtimestamp(one_day_ago)}
        }).limit(200))
        
        db_raw["total_jobs_24h"] = len(recent_jobs)
        db_raw["stage_dist"] = {}
        
        # [NEW] ë¯¸ì‹œì  í’ˆì§ˆ ì§€í‘œ ê³„ì‚° (Custom Definitions ëŒ€ì²´/ë³´ì™„)
        stability_scores = []
        gen_times = []
        brick_counts = []
        error_dist = {}
        input_type_dist = {}
        
        for j in recent_jobs:
            st = j.get("stage", "UNKNOWN")
            db_raw["stage_dist"][st] = db_raw["stage_dist"].get(st, 0) + 1
            
            # ì•ˆì •ì„± ì ìˆ˜ (result.stabilityScore)
            if j.get("result") and "stabilityScore" in j["result"]:
                stability_scores.append(j["result"]["stabilityScore"])
                
            # ìƒì„± ì†Œìš” ì‹œê°„ (endedAt - startedAt)
            if j.get("startedAt") and j.get("endedAt"):
                try:
                    dur = (j["endedAt"] - j["startedAt"]).total_seconds()
                    if 0 < dur < 600: # 10ë¶„ ì´ìƒì€ ì´ìƒì¹˜ ì œì™¸
                        gen_times.append(dur)
                except: pass
                
            # ë¸Œë¦­ ê°œìˆ˜ (result.brickCount)
            if j.get("result") and "brickCount" in j["result"]:
                brick_counts.append(j["result"]["brickCount"])
            
            # ì—ëŸ¬ ìœ í˜• ë¶„í¬ (ì‹¤íŒ¨ ì›ì¸ ë¶„ì„ìš©)
            if j.get("error"):
                # ì—ëŸ¬ ë©”ì‹œì§€ë‚˜ ì½”ë“œë¥¼ ë‹¨ìˆœí™”í•´ì„œ ì¹´ìš´íŒ…
                err_msg = str(j["error"])[:50] 
                error_dist[err_msg] = error_dist.get(err_msg, 0) + 1
            
            # ì…ë ¥ ë°©ì‹ ì„ í˜¸ë„ (Text Prompt vs Image Upload)
            inp = j.get("inputType", "unknown")
            input_type_dist[inp] = input_type_dist.get(inp, 0) + 1

        # í‰ê· ê°’ ë° ë¶„í¬ ì‚°ì¶œ
        db_raw["avg_stability"] = round(sum(stability_scores) / len(stability_scores), 2) if stability_scores else 0.0
        db_raw["avg_gen_time"] = round(sum(gen_times) / len(gen_times), 1) if gen_times else 0.0
        db_raw["avg_brick_count"] = int(sum(brick_counts) / len(brick_counts)) if brick_counts else 0
        db_raw["error_dist"] = error_dist
        db_raw["input_type_dist"] = input_type_dist
            
        log.info(f"[Miner] DB ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: Jobs={len(recent_jobs)} (AvgStability={db_raw['avg_stability']})")
    except Exception as e:
        log.warning(f"[Miner] DB ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ì§„í–‰): {e}")

    now = datetime.now()
    temporal = {
        "day_of_week": now.strftime("%a"),
        "is_weekend": now.weekday() >= 5,
        "hour": now.hour,
        "is_peak": 19 <= now.hour <= 23,
        "date": now.strftime("%Y-%m-%d"),
    }

    log.info(f"[Miner] ìˆ˜ì§‘ ì™„ë£Œ: summary={bool(summary)}, db_raw={bool(db_raw)}, today_gen={bool(today_gen_success)}")

    return {
        "raw_metrics": {
            "summary": summary or {},
            "daily_users": daily or [],
            "top_tags": tags or [],
            "heavy_users": users or [],
            "fail_events": fail_7d or [],       # [ë³µêµ¬] Evaluatorìš©
            "success_events": success_7d or [], # [ë³µêµ¬] Evaluatorìš©
            "db_raw": db_raw,
            "today_stats": {
                "gen_success": sum(e.get("count", 0) for e in (today_gen_success or [])),
                "gen_fail": sum(e.get("count", 0) for e in (today_gen_fail or [])),
                "gallery_uploads": sum(e.get("count", 0) for e in (today_gallery or [])),
            },
            "top_posts": top_posts or [],
        },
        "temporal_context": temporal,
        "moderation_queue": [],
        "moderation_results": [],
        "next_action": "evaluate",
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Node 2: Evaluator â€” ì´ìƒ íƒì§€ (ê·œì¹™ ê¸°ë°˜, LLM ë¯¸ì‚¬ìš©)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def evaluator_node(state: AdminAnalystState) -> dict:
    """Z-Score ë° DB í’ˆì§ˆ ì§€í‘œ ê¸°ë°˜ ì´ìƒ íƒì§€."""
    log.info("[Evaluator] ì´ìƒ íƒì§€ ì‹œì‘...")
    anomalies: List[Dict[str, Any]] = []
    metrics = state.get("raw_metrics", {})
    db_raw = metrics.get("db_raw", {})

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚  CHECK 1: Macro Analytics Anomalies (DAU, Fail Rare)        â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    # â”€â”€ 1-A. DAU ê¸‰ë³€ ê°ì§€ â”€â”€
    daily = metrics.get("daily_users") or []
    dau_spike = False # ë§ˆì¼€íŒ… ê°ì§€ìš© í”Œë˜ê·¸
    if len(daily) >= 3:
        try:
            counts = [d.get("count", d.get("activeUsers", 0)) for d in daily]
            prev, today = counts[:-1], counts[-1]
            mean = sum(prev) / len(prev) if prev else 0
            std = (sum((x - mean) ** 2 for x in prev) / len(prev)) ** 0.5 if prev else 0

            if std > 0:
                z = (today - mean) / std
                if abs(z) > 2.0:
                    severity = "HIGH" if abs(z) > 3.5 else "MEDIUM"
                    direction = "DROP" if z < 0 else "SPIKE"
                    if direction == "SPIKE":
                        dau_spike = True
                        
                    anomalies.append({
                        "metric": "daily_active_users",
                        "current": today,
                        "baseline": round(mean, 1),
                        "severity": severity,
                        "z_score": round(z, 2),
                        "direction": direction,
                    })
        except Exception as e:
            log.warning(f"[Evaluator] DAU ë¶„ì„ ì˜¤ë¥˜: {e}")

    # â”€â”€ 1-B. ìƒì„± ì‹¤íŒ¨ìœ¨ ê¸‰ì¦ â”€â”€
    fail_ev = metrics.get("fail_events") or []
    succ_ev = metrics.get("success_events") or []
    today_failures = metrics.get("today_stats", {}).get("gen_fail", 0)
    
    if fail_ev and succ_ev:
        try:
            fc = [e.get("count", 0) for e in fail_ev]
            # ì˜¤ëŠ˜ ë°ì´í„°ê°€ API ê°±ì‹  ì „ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì‹¤ì‹œê°„ today_stats ìš°ì„  ê³ ë ¤
            if today_failures > 0:
                recent_fail = today_failures
            else:
                recent_fail = sum(fc[-1:]) if fc else 0
            
            recent_succ = metrics.get("today_stats", {}).get("gen_success", 0)
            total = recent_fail + recent_succ

            if total > 5:
                rate = recent_fail / total
                prev_rate_avg = 0.1 # ê¸°ë³¸ê°’
                
                if rate > 0.2: # 20% ì´ìƒ ì‹¤íŒ¨ ì‹œ ì²´í¬
                    anomalies.append({
                        "metric": "generation_fail_rate",
                        "current": round(rate * 100, 1),
                        "baseline": "10.0",
                        "severity": "HIGH" if rate > 0.4 else "MEDIUM",
                        "z_score": round(rate / 0.1, 2),
                        "direction": "SPIKE",
                    })
        except Exception as e:
            log.warning(f"[Evaluator] ì‹¤íŒ¨ìœ¨ ë¶„ì„ ì˜¤ë¥˜: {e}")

    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚  CHECK 2: Micro DB Anomalies (Quality, Latency, Marketing)  â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    # â”€â”€ 2-A. [NEW] í‰ê·  ì•ˆì •ì„± ì ìˆ˜ í•˜ë½ (0.7 ë¯¸ë§Œì´ë©´ ì£¼ì˜) â”€â”€
    avg_stability = db_raw.get("avg_stability", 0.0)
    if avg_stability > 0 and avg_stability < 0.7:
        anomalies.append({
            "metric": "avg_stability_score",
            "current": avg_stability,
            "baseline": 0.85,
            "severity": "HIGH" if avg_stability < 0.5 else "MEDIUM",
            "direction": "DROP",
            "desc": "ìƒì„±ëœ ë¸Œë¦­ì˜ ë¬¼ë¦¬ì  ì•ˆì •ì„±ì´ í¬ê²Œ ë–¨ì–´ì§"
        })

    # â”€â”€ 2-B. [NEW] ìƒì„± ì‹œê°„ ì§€ì—° (í‰ê·  60ì´ˆ ì´ˆê³¼ ì‹œ ì£¼ì˜) â”€â”€
    avg_gen_time = db_raw.get("avg_gen_time", 0.0)
    if avg_gen_time > 60:
        anomalies.append({
            "metric": "avg_generation_time",
            "current": f"{avg_gen_time}s",
            "baseline": "30s",
            "severity": "HIGH" if avg_gen_time > 120 else "MEDIUM",
            "direction": "DELAY",
            "desc": "AI ì—”ì§„ ì²˜ë¦¬ ì†ë„ ì €í•˜ ê°ì§€"
        })

    # â”€â”€ 2-C. [NEW] ë§ˆì¼€íŒ… íš¨ìœ¨/íŠ¸ë˜í”½ í’ˆì§ˆ ê°ì§€ â”€â”€
    # DAUëŠ” ê¸‰ì¦í–ˆëŠ”ë°(SPIKE), ìƒì„± ì‹œë„ëŠ” ëŠ˜ì§€ ì•Šì•˜ë‹¤ë©´ í—ˆìˆ˜ ìœ ì… ê°€ëŠ¥ì„±
    if dau_spike:
        total_gens = db_raw.get("total_jobs_24h", 0)
        # í‰ì†Œ 100ëª…ë‹¹ 10ê°œ ìƒì„±í•œë‹¤ê³  ê°€ì • (10%)
        # íŠ¸ë˜í”½ ëŒ€ë¹„ ìƒì„± ë¹„ìœ¨ì´ ë„ˆë¬´ ë‚®ìœ¼ë©´ ë§ˆì¼€íŒ… íš¨ìœ¨ ì €í•˜ë¡œ ì˜ì‹¬
        daily_count = metrics.get("daily_users", [])[-1].get("activeUsers", 1) if metrics.get("daily_users") else 1
        conversion_rate = total_gens / max(daily_count, 1)
        
        if conversion_rate < 0.05: # 5% ë¯¸ë§Œì´ë©´ ì²´ë¦¬í”¼ì»¤ ìœ ì… ì˜ì‹¬
            anomalies.append({
                "metric": "traffic_quality_drop",
                "current": f"{round(conversion_rate*100, 1)}%",
                "baseline": "10.0%",
                "severity": "MEDIUM",
                "direction": "DROP",
                "desc": "íŠ¸ë˜í”½ ê¸‰ì¦ ëŒ€ë¹„ ì‹¤ì œ ì‚¬ìš© ì „í™˜ìœ¨ ì €ì¡° (ì €í’ˆì§ˆ ìœ ì…/ë§ˆì¼€íŒ… íš¨ìœ¨ ì˜ì‹¬)"
            })


    # â”€â”€ 3. ì¢…í•© ìœ„í—˜ ì ìˆ˜ (Threshold Tuned) â”€â”€
    # HIGH = 0.5 (í•˜ë‚˜ë§Œ ìˆì–´ë„ ì¦‰ì‹œ ë¦¬í¬íŠ¸ ì „í™˜)
    # MEDIUM = 0.2 (ìµœì†Œ 3ê°œëŠ” ëª¨ì—¬ì•¼ ë¦¬í¬íŠ¸ ì „í™˜)
    # Threshold = 0.5
    risk = sum(0.5 if a["severity"] == "HIGH" else 0.2 for a in anomalies)
    risk = min(1.0, risk)
    
    next_step = "diagnose" if risk >= 0.5 else "report_green"

    log.info(f"[Evaluator] ì™„ë£Œ: {len(anomalies)}ê±´ ì´ìƒ, risk={risk} â†’ {next_step}")

    return {
        "anomalies": anomalies,
        "risk_score": risk,
        "next_action": next_step,
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Node 3: Diagnoser â€” ì¸ê³¼ê´€ê³„ ì¶”ë¡  (LLM)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def diagnoser_node(state: AdminAnalystState) -> dict:
    """LLMìœ¼ë¡œ ì´ìƒ ì§•í›„ì˜ ê·¼ë³¸ ì›ì¸ì„ ì¶”ë¡ ."""
    log.info("[Diagnoser] ì›ì¸ ì¶”ë¡  ì‹œì‘...")

    anomaly_text = json.dumps(state.get("anomalies", []), ensure_ascii=False, indent=2)
    summary = state.get("raw_metrics", {}).get("summary", {})
    db_raw = state.get("raw_metrics", {}).get("db_raw", {})
    temporal = state.get("temporal_context", {})
    tags = state.get("raw_metrics", {}).get("top_tags", [])

    prompt = DIAGNOSER_PROMPT.format(
        anomaly_text=anomaly_text,
        active_users=summary.get('activeUsers', 'N/A'),
        page_views=summary.get('pageViews', 'N/A'),
        sessions=summary.get('sessions', 'N/A'),
        total_jobs=db_raw.get('total_jobs_24h', 0),
        avg_stability=db_raw.get('avg_stability', 0.0),
        avg_gen_time=db_raw.get('avg_gen_time', 0.0),
        stage_dist=json.dumps(db_raw.get('stage_dist', {}), ensure_ascii=False),
        error_dist=json.dumps(db_raw.get('error_dist', {}), ensure_ascii=False),
        input_type_dist=json.dumps(db_raw.get('input_type_dist', {}), ensure_ascii=False),
        top_tags=json.dumps(tags[:10], ensure_ascii=False),
        date=temporal.get('date'),
        hour=temporal.get('hour'),
        day_of_week=temporal.get('day_of_week')
    )

    diagnosis = await call_llm_json(prompt)

    if not diagnosis:
        diagnosis = {
            "root_cause": "LLM ë¶„ì„ ì‹¤íŒ¨ â€” ìˆ˜ë™ í™•ì¸ í•„ìš”",
            "confidence": 0.3,
            "evidence": [f"{len(state.get('anomalies', []))}ê±´ ì´ìƒ ì§•í›„ ê°ì§€"],
            "affected_segment": "ì „ì²´ ìœ ì €",
        }

    log.info(f"[Diagnoser] ì™„ë£Œ: {diagnosis.get('root_cause', '')[:50]}...")
    return {"diagnosis": diagnosis, "next_action": "strategize"}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Node 4: Strategist â€” ëŒ€ì‘ ì „ëµ ìˆ˜ë¦½ (LLM)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def strategist_node(state: AdminAnalystState) -> dict:
    """ì§„ë‹¨ ê¸°ë°˜ êµ¬ì²´ ëŒ€ì‘ ì „ëµ ë„ì¶œ."""
    log.info("[Strategist] ì „ëµ ìˆ˜ë¦½ ì‹œì‘...")

    dx = state.get("diagnosis", {})
    confidence = dx.get("confidence", 0.5)

    prompt = STRATEGIST_PROMPT.format(
        root_cause=dx.get('root_cause', '?'),
        risk_level=dx.get('risk_level', 'UNKNOWN'),
        confidence=confidence * 100,
        evidence=json.dumps(dx.get('evidence', []), ensure_ascii=False)
    )

    actions = await call_llm_json(prompt)

    if isinstance(actions, dict):
        actions = [actions]
    if not isinstance(actions, list):
        actions = [{"action": "ìˆ˜ë™ ëª¨ë‹ˆí„°ë§ ê°•í™”", "target": "ì„œë¹„ìŠ¤ ì „ì²´",
                     "expected_impact": "ì‹¤ì‹œê°„ íŒŒì•…", "risk": "LOW", "priority": 1}]

    iteration = state.get("iteration", 0) + 1

    # í™•ì‹ ë„ ë‚®ìœ¼ë©´ ë£¨í”„ë°±
    if confidence < 0.5 and iteration < state.get("max_iterations", 3):
        log.info(f"[Strategist] í™•ì‹ ë„ {confidence} < 0.5 â†’ ì‹¬ì¸µ ì¡°ì‚¬ (iter={iteration})")
        return {"proposed_actions": actions, "iteration": iteration, "next_action": "deep_investigate"}

    return {"proposed_actions": actions, "iteration": iteration, "next_action": "finalize"}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Node 8: Content Miner â€” ê²€ì—´ ëŒ€ìƒ ìˆ˜ì§‘
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def content_miner_node(state: AdminAnalystState) -> dict:
    """ë°±ì—”ë“œì—ì„œ ì•„ì§ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ìµœê·¼ ëŒ“ê¸€/ê²Œì‹œê¸€ ìˆ˜ì§‘."""
    from service import backend_client
    log.info("[ContentMiner] ê²€ì—´ ëŒ€ìƒ ìˆ˜ì§‘ ì‹œì‘...")

    # ìµœê·¼ 1ì¼ ë‚´ì˜ ë¯¸ê²€ì—´ ì½˜í…ì¸  ìµœëŒ€ 50ê°œ ìˆ˜ì§‘
    contents = await backend_client.get_recent_contents(days=1, limit=50)

    log.info(f"[ContentMiner] ìˆ˜ì§‘ ì™„ë£Œ: {len(contents or [])}ê±´")
    return {
        "moderation_queue": contents or [],
        "next_action": "guard"
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Node 9: Guardian â€” ì„ ì •ì„±/í­ë ¥ì„± íŒë‹¨ (LLM)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def guardian_node(state: AdminAnalystState) -> dict:
    """LLMì„ ë¸”ë™ë°•ìŠ¤ ê²€ì—´ê´€ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ë¶€ì ì ˆì„± íŒë‹¨."""
    log.info("[Guardian] ì½˜í…ì¸  ê²€ì—´ ì‹œì‘...")

    queue = state.get("moderation_queue", [])
    if not queue:
        return {"next_action": "execute_moderation"}

    queue_text = json.dumps(queue, ensure_ascii=False, indent=2)

    prompt = GUARDIAN_PROMPT.format(queue_text=queue_text)

    judgments = await call_llm_json(prompt)
    if not isinstance(judgments, list):
        judgments = []

    log.info(f"[Guardian] ê²€ì—´ ì™„ë£Œ: {len(judgments)}ê±´ íŒì •")
    return {
        "moderation_results": judgments,
        "next_action": "execute_moderation"
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Node 10: Moderator Executor â€” ìë™ ì¡°ì¹˜ ì‹¤í–‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def moderator_executor_node(state: AdminAnalystState) -> dict:
    """Guardianì˜ íŒë‹¨ì— ë”°ë¼ ë°±ì—”ë“œì— ìë™ ìˆ¨ê¹€ ì²˜ë¦¬ ìš”ì²­."""
    from service import backend_client
    log.info("[ModeratorExecutor] ìë™ ì¡°ì¹˜ ì‹¤í–‰ ì‹œì‘...")

    results = state.get("moderation_results", [])
    executed_count = 0

    for res in results:
        if res.get("is_violating") and res.get("confidence", 0) >= 0.8:
            target_id = res.get("target_id")
            target_type = res.get("type")
            reason = res.get("reason", "AI Automated Moderation")

            success = await backend_client.hide_content(target_type, target_id, reason)
            if success:
                executed_count += 1
                res["action_taken"] = "HIDDEN"
                log.info(f"[ModeratorExecutor] ì¡°ì¹˜ ì™„ë£Œ: {target_type} {target_id}")
            else:
                res["action_taken"] = "FAILED"

    log.info(f"[ModeratorExecutor] ì´ {executed_count}ê±´ ìë™ ì¡°ì¹˜ ì™„ë£Œ")
    return {
        "moderation_results": results,
        "next_action": "finalize"
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Node 5: Deep Investigator â€” ì‹¬ì¸µ ì¡°ì‚¬ (ë£¨í”„ë°±)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def deep_investigator_node(state: AdminAnalystState) -> dict:
    """30ì¼ ë°ì´í„° ì¶”ê°€ ìˆ˜ì§‘ í›„ Diagnoserë¡œ ë£¨í”„ë°±."""
    from service import backend_client

    log.info("[DeepInvestigator] 30ì¼ ì¥ê¸° ë°ì´í„° ìˆ˜ì§‘...")

    long_daily = await backend_client.get_daily_users(30)
    long_tags = await backend_client.get_top_tags(30, limit=20)

    metrics = dict(state.get("raw_metrics", {}))
    metrics["daily_users_30d"] = long_daily or []
    metrics["top_tags_30d"] = long_tags or []

    return {"raw_metrics": metrics, "next_action": "diagnose"}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Node 6: Reporter Green â€” ì •ìƒ ë³´ê³ ì„œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def reporter_green_node(state: AdminAnalystState) -> dict:
    """ì´ìƒ ì§•í›„ê°€ ì—†ì„ ë•Œë„ LLMìœ¼ë¡œ ì‹¬ì¸µ ìš´ì˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±."""
    log.info("[Reporter] ì •ìƒ ìƒíƒœ ì‹¬ì¸µ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹œì‘...")

    metrics = state.get("raw_metrics", {})
    summary = metrics.get("summary", {})
    daily = metrics.get("daily_users", [])
    tags = metrics.get("top_tags", [])
    temporal = state.get("temporal_context", {})

    # íŠ¸ë Œë“œ ìš”ì•½ (LLM ì°¸ê³ ìš©)
    trend_desc = "ë³´í•©ì„¸"
    if daily and len(daily) >= 3:
        try:
            counts = [d.get("count", d.get("activeUsers", 0)) for d in daily]
            recent_avg = sum(counts[-3:]) / 3
            prev_avg = sum(counts[-6:-3]) / 3 if len(counts) >= 6 else counts[0]
            chg = ((recent_avg - prev_avg) / max(prev_avg, 1)) * 100
            trend_desc = f"ìµœê·¼ 3ì¼ í‰ê· ì´ ì´ì „ ëŒ€ë¹„ {chg:+.1f}% {'ìƒìŠ¹' if chg > 0 else 'í•˜ë½'} ì¤‘"
        except: pass

    prompt = REPORTER_GREEN_PROMPT.format(
        active_users=summary.get('activeUsers', 'N/A'),
        page_views=summary.get('pageViews', 'N/A'),
        sessions=summary.get('sessions', 'N/A'),
        trend_desc=trend_desc,
        top_tags=', '.join(f"#{t.get('tag', 'ì•Œìˆ˜ì—†ìŒ')}" for t in tags[:7]),
        day_of_week=temporal.get('day_of_week'),
        hour=temporal.get('hour'),
        is_peak=temporal.get('is_peak')
    )

    res = await call_llm_json(prompt)
    report = res.get("report") if res else None

    if not report:
        report = f"## âœ… ì„œë¹„ìŠ¤ ì•ˆì • ìš´ì˜ ì¤‘\n\nëª¨ë“  í•µì‹¬ ì§€í‘œê°€ ì •ìƒ ë²”ìœ„ë¥¼ ìœ ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ìœ ì € ìœ ì… ë° ì „í™˜ íŠ¸ë Œë“œê°€ ì•ˆì •ì ì…ë‹ˆë‹¤. ({trend_desc})"

    return {"final_report": report, "next_action": "end"}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Node 7: Finalizer â€” ì´ìƒ ë°œê²¬ ì‹œ ì¢…í•© ë³´ê³ ì„œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def finalizer_node(state: AdminAnalystState) -> dict:
    """ì´ìƒ ì§•í›„ ë°œê²¬ ì‹œ LLMìœ¼ë¡œ ìœ ê¸°ì ì¸ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±."""
    log.info("[Finalizer] ì¢…í•© ë³´ê³ ì„œ ìƒì„± ì‹œì‘...")

    dx = state.get("diagnosis", {})
    actions = state.get("proposed_actions", [])
    anomalies = state.get("anomalies", [])
    mod_results = state.get("moderation_results", [])
    temporal = state.get("temporal_context", {})

    # ìë™ ì¡°ì¹˜ ë‚´ì—­ ìš”ì•½
    hidden_count = sum(1 for r in mod_results if r.get("action_taken") == "HIDDEN")
    mod_text = ""
    if mod_results:
        mod_text = "\n### ğŸ›¡ï¸ ììœ¨ ì½˜í…ì¸  ê²€ì—´ ë° ì¡°ì¹˜ ë‚´ì—­\n"
        if hidden_count > 0:
            mod_text += f"- **ìë™ ìˆ¨ê¹€ ì²˜ë¦¬**: {hidden_count}ê±´ (AI í™•ì‹ ë„ 80% ì´ìƒ)\n"
        else:
            mod_text += "- íŠ¹ì´ì‚¬í•­: ìœ„ë°˜ ì˜ì‹¬ ì½˜í…ì¸  ì—†ìŒ (í´ë¦° ìƒíƒœ ìœ ì§€ ì¤‘)\n"

        for r in [r for r in mod_results if r.get("is_violating")][:5]:
             mod_text += f"  - [{r.get('violation_type')}] {r.get('target_id')}: {r.get('reason')} ({r.get('action_taken', 'PENDING')})\n"

    prompt = FINALIZER_PROMPT.format(
        anomalies=json.dumps(anomalies, ensure_ascii=False, indent=2),
        mod_text=mod_text,
        root_cause=dx.get('root_cause', '?'),
        forecast=dx.get('forecast', 'ë°ì´í„° ìˆ˜ì§‘ ì¤‘...'),
        confidence=dx.get('confidence', 0) * 100,
        evidence=json.dumps(dx.get('evidence', []), ensure_ascii=False),
        affected_segment=dx.get('affected_segment', 'ì „ì²´'),
        actions=json.dumps(actions, ensure_ascii=False, indent=2)
    )

    res = await call_llm_json(prompt)
    report = res.get("report") if res else None

    if not report:
        # Fallback í…œí”Œë¦¿
        report = f"## ğŸš¨ ê´€ë¦¬ì ì£¼ì˜: ì´ìƒ ì§•í›„ ê°ì§€\n\n- ì›ì¸: {dx.get('root_cause', '?')}\n- ì¡°ì¹˜: {len(actions)}ê±´ì˜ ì „ëµ ìˆ˜ë¦½ë¨."

    return {"final_report": report, "next_action": "end"}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Node 11: Query Analyst â€” ì¸í„°ë™í‹°ë¸Œ ì§ˆì˜ì‘ë‹µ (NEW)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
async def query_analyst_node(state: AdminAnalystState) -> dict:
    """ì „ëµì  AI ì–´ë“œë°”ì´ì €: ëŒ€í™” ì´ë ¥ê³¼ 100ì—¬ ì§€í‘œë¥¼ ì¢…í•©í•˜ì—¬ ì…ì²´ì  ì „ëµ ìˆ˜ë¦½."""
    log.info("[QueryAnalyst] ëŒ€í™” ë§¥ë½ í¬í•¨ ì „ëµ ë¶„ì„ ì‹œì‘...")

    user_query = state.get("user_query", "í˜„ì¬ ì„œë¹„ìŠ¤ ìš´ì˜ ìƒíƒœ ì¢…í•© ì§„ë‹¨")
    history = state.get("history", []) # [NEW] ëŒ€í™” ì´ë ¥
    metrics = state.get("raw_metrics", {})
    summary = metrics.get("summary", {})
    daily = metrics.get("daily_users", [])
    tags = metrics.get("top_tags", [])
    today = metrics.get("today_stats", {})
    db_raw = metrics.get("db_raw", {})
    top_posts = metrics.get("top_posts", [])
    temporal = state.get("temporal_context", {})

    # ì´ì „ ëŒ€í™” ë§¥ë½ ìš”ì•½
    history_context = ""
    if history:
        history_context = "\n[ì´ì „ ëŒ€í™” ë§¥ë½]\n" + "\n".join([f"{h['role']}: {h['content']}" for h in history[-3:]])

    prompt = QUERY_ANALYST_PROMPT.format(
        history_context=history_context,
        user_query=user_query,
        summary=json.dumps(summary, ensure_ascii=False),
        today_gen_success=today.get('gen_success'),
        today_gen_fail=today.get('gen_fail'),
        today_gallery=today.get('gallery_uploads'),
        total_jobs=db_raw.get('total_jobs_24h'),
        stage_dist=json.dumps(db_raw.get('stage_dist', {}), ensure_ascii=False),
        daily=json.dumps(daily, ensure_ascii=False),
        tags=json.dumps(tags[:10], ensure_ascii=False),
        top_posts=json.dumps(top_posts, ensure_ascii=False),
        temporal=json.dumps(temporal, ensure_ascii=False)
    )

    res = await call_llm_json(prompt)
    report = res.get("report") if isinstance(res, dict) else str(res)
    
    if not report or report == "None":
        from .llm_utils import call_llm_text
        report = await call_llm_text(prompt)

    # ì´ë ¥ ì—…ë°ì´íŠ¸ëŠ” í˜¸ì¶œë¶€ì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ ì œì•ˆ (í˜„ì¬ ë…¸ë“œì—ì„œëŠ” ê²°ê³¼ë§Œ ë°˜í™˜)
    return {"final_report": report, "next_action": "end"}
