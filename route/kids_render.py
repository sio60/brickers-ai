# brickers-ai/route/kids_render.py
"""Kids Mode ë¼ìš°í„° + ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ (ë¦¬íŒ©í† ë§ ë²„ì „)"""
from __future__ import annotations

import os
import json
import re
import uuid
import base64
import time
import traceback
from pathlib import Path
from typing import Dict, Optional, Any
from datetime import datetime
import sys
import io
import asyncio
from typing import TextIO

from service.log_context import JobLogContext

# [REMOVED] Local LogCapture Class (Replaced by GlobalLogCapture + JobLogContext)

import anyio
import httpx
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

from tripo3d import TripoClient
from tripo3d.models import TaskStatus

# --- Service modules ---
from service.kids_config import (
    GENERATED_DIR,
    KIDS_TOTAL_TIMEOUT_SEC,
    TRIPO_WAIT_TIMEOUT_SEC,
    DOWNLOAD_TIMEOUT_SEC,
    AGE_TO_BUDGET,
    budget_to_start_target,
    DEBUG,
)
from service.s3_client import USE_S3, S3_BUCKET, to_generated_url, upload_bytes_to_s3
from service.gemini_image import render_one_image_async
from service.backend_client import (
    update_job_stage,
    update_job_suggested_tags,
    make_agent_log_sender,
    send_agent_log,
)
from service.bom_generator import generate_bom_from_ldr
from service.brickify_loader import (
    load_engine_convert,
    load_agent_modules,
    find_glb_in_dir,
    pick_glb_from_downloaded,
)

# PDF Generation (SQSë¡œ Blueprint ì„œë²„ì— ìœ„ì„)
from route.sqs_producer import send_pdf_request_message, send_screenshot_request_message

# from brick_engine.agent.log_analyzer.persistence import archive_job_logs # [DECOUPLED]
from brick_engine.agent.log_analyzer.persistence import archive_job_logs

# Re-export for app.py / sqs_consumer.py
__all__ = ["router", "GENERATED_DIR", "process_kids_request_internal"]

router = APIRouter(prefix="/api/v1/kids", tags=["kids"])

# --- Engine cache ---
_CONVERT_FN = None

def log(msg: str, user_email: str = "System") -> None:
    """íƒ€ì„ìŠ¤íƒ¬í”„ ë° ì‚¬ìš©ì ì •ë³´ í¬í•¨ ë¡œê·¸ ì¶œë ¥"""
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
    # ê¸´ ì´ë©”ì¼ì€ ì•ë¶€ë¶„ë§Œ ì¶œë ¥
    user_tag = f"[{user_email}]" if user_email else "[System]"
    print(f"[{ts}] {user_tag} {msg}", flush=True)


# --------------- helpers ---------------

async def _write_bytes_async(path: Path, data: bytes) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    await anyio.to_thread.run_sync(path.write_bytes, data)


async def _read_bytes_async(path: Path) -> bytes:
    return await anyio.to_thread.run_sync(path.read_bytes)


def _write_error_log(out_dir: Path, text: str) -> None:
    try:
        out_dir.mkdir(parents=True, exist_ok=True)
        (out_dir / "error.log").write_text(text, encoding="utf-8")
    except Exception:
        pass


async def _download_http_to_file(url: str, dst: Path) -> Path:
    dst.parent.mkdir(parents=True, exist_ok=True)
    async with httpx.AsyncClient(timeout=DOWNLOAD_TIMEOUT_SEC, follow_redirects=True) as client:
        r = await client.get(url)
        r.raise_for_status()
        await _write_bytes_async(dst, r.content)
    return dst


async def _download_from_s3(url: str) -> bytes:
    async with httpx.AsyncClient(timeout=120.0, follow_redirects=True) as client:
        resp = await client.get(url)
        resp.raise_for_status()
        return resp.content


def _pick_model_url(files_url: Dict[str, str]) -> str:
    for k, u in files_url.items():
        if "glb" in k.lower() or u.lower().endswith(".glb"):
            return u
    for k, u in files_url.items():
        if "pbr" in k.lower():
            return u
    return next(iter(files_url.values()))


def _sanitize_glb_url(u: str) -> str:
    u = (u or "").strip()
    if u.startswith("/api/api/"):
        u = u.replace("/api/api/", "/api/", 1)
    return u


def _local_generated_path_from_url(u: str) -> Optional[Path]:
    u = _sanitize_glb_url(u)
    if u.startswith("/api/generated/"):
        rel = u[len("/api/generated/"):]
        return (GENERATED_DIR / rel).resolve()
    if u.startswith("/generated/"):
        rel = u[len("/generated/"):]
        return (GENERATED_DIR / rel).resolve()
    return None



# --------------- Request / Response ---------------

class KidsProcessRequest(BaseModel):
    sourceImageUrl: str
    userEmail: Optional[str] = "unknown@brickers.shop"  # [ì¶”ê°€]
    age: str = "6-7"
    budget: Optional[int] = None
    subject: Optional[str] = None
    prompt: Optional[str] = None
    returnLdrData: bool = False


class ProcessResp(BaseModel):
    ok: bool
    reqId: str
    correctedUrl: str
    taskId: str
    modelUrl: str
    files: Dict[str, str]
    ldrUrl: str
    ldrData: Optional[str] = None
    bomUrl: str
    pdfUrl: Optional[str] = None # [New]
    subject: str
    tags: list[str]
    parts: int
    finalTarget: int


# --------------- Core orchestration ---------------

async def process_kids_request_internal(
    job_id: str,
    source_image_url: str,
    age: str,
    budget: Optional[int] = None,
    subject: Optional[str] = None,
    user_email: str = "unknown",
    external_log_buffer: Optional[list[str]] = None, # [NEW]
) -> Dict[str, Any]:
    """
    Kids ë Œë”ë§ ë‚´ë¶€ ë¡œì§ (SQS Consumerì—ì„œ í˜¸ì¶œ)
    """
    total_start = time.time()
    
    # ë‚´ë¶€ ë˜í¼ ë¡œê·¸ (ì´ë©”ì¼ ìë™ ì£¼ì… + Job ID íƒœê¹…)
    def _log(msg: str):
        log(f"[{job_id}] {msg}", user_email=user_email)

    TRIPO_API_KEY = os.environ.get("TRIPO_API_KEY", "")
    if not TRIPO_API_KEY:
        raise RuntimeError("TRIPO_API_KEY is not set")
    
    # [NEW] In-Memory Log Buffer (Use external if provided)
    if external_log_buffer is not None:
        job_log_buffer = external_log_buffer
    else:
        job_log_buffer = []

    req_id = job_id
    out_req_dir = GENERATED_DIR / f"req_{req_id}"
    out_tripo_dir = GENERATED_DIR / f"tripo_{req_id}"
    out_brick_dir = GENERATED_DIR / f"brickify_{req_id}"
    out_req_dir.mkdir(parents=True, exist_ok=True)
    out_tripo_dir.mkdir(parents=True, exist_ok=True)
    out_brick_dir.mkdir(parents=True, exist_ok=True)

    # ë‚´ë¶€ ë˜í¼ ë¡œê·¸ (ì´ë©”ì¼ ìë™ ì£¼ì… + ë²„í¼ë§)
    # [ìˆ˜ì •] LogCaptureê°€ ì´ë¯¸ stdoutì„ ìº¡ì³í•˜ë¯€ë¡œ, 
    # ì—¬ê¸°ì„œëŠ” print()ë§Œ ì°ìœ¼ë©´ Dockerì™€ Buffer ì–‘ìª½ìœ¼ë¡œ ë“¤ì–´ê°.
    # ë‹¨, log() í•¨ìˆ˜ ìì²´ê°€ print()ë¥¼ í˜¸ì¶œí•˜ë¯€ë¡œ ì¤‘ë³µ ì €ì¥ì„ ë§‰ê¸° ìœ„í•´
    # log()ëŠ” ê·¸ëŒ€ë¡œ ë‘ê³ , LogCaptureê°€ ì•Œì•„ì„œ ìº¡ì³í•˜ê²Œ ë‘ .
    def _log(msg: str):
        # 1. ì½˜ì†” ì¶œë ¥ -> LogCaptureê°€ ê°€ë¡œì±„ì„œ Bufferì—ë„ ë„£ìŒ
        log(f"[{job_id}] {msg}", user_email=user_email)
        
    # --- SSE ì‹¤ì‹œê°„ ë¡œê·¸ ì „ì†¡ìš© (async) ---
    async def _sse(step: str, message: str = ""):
        """íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ SSE ë¡œê·¸ ì „ì†¡ (async httpx)"""
        await send_agent_log(job_id, step, message)

    _log("\u2550" * 70)
    _log(f"\U0001f680 [AI-SERVER] \uc694\uccad \uc2dc\uc791") # jobId ì¤‘ë³µ ì œê±°
    _log(f"\U0001f4c1 \uc6d0\ubcf8 \uc774\ubbf8\uc9c0 URL: {source_image_url}")
    _log(f"\U0001f4ca \ud30c\ub77c\ubbf8\ud130: subject={subject} | age={age} | budget={budget}")
    s3_label = "ON" if USE_S3 else "OFF"
    _log(f"\u2699\ufe0f  S3 \ubaa8\ub4dc: {s3_label} | bucket={S3_BUCKET or 'N/A'}")
    _log("\u2550" * 70)
    
    # [DECOUPLED] Manual archives removed. SQS Consumer handles auto-flushing.
    # [Restored as comments per user request]
    # async def _async_archive(status: str = "RUNNING"):
    #     try:
    #         await archive_job_logs(job_id, list(job_log_buffer), status=status)
    #     except Exception as e:
    #         print(f"âš ï¸ [LogArchive] Failed: {e}")

    try:
        # [CHANGE] Global Context ì‚¬ìš©
        with JobLogContext(job_log_buffer):
            # ì´ˆê¸° start ë¡œê·¸ë„ ìº¡ì³ë¨
            # await _async_archive() # [Restored comment]
            
            with anyio.fail_after(KIDS_TOTAL_TIMEOUT_SEC):

                # 0) S3ì—ì„œ ì›ë³¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                step_start = time.time()
                _log("\U0001f4cc [STEP 0/5] S3\uc5d0\uc11c \uc6d0\ubcf8 \uc774\ubbf8\uc9c0 \ub2e4\uc6b4\ub85c\ub4dc \uc911...")
                await _sse("download", "ì´ë¯¸ì§€ ìˆ˜ì‹  ì™„ë£Œ. êµ¬ì¡°ë¶€í„° ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.")
                img_bytes = await _download_from_s3(source_image_url)
                raw_path = out_req_dir / "raw.png"
                await _write_bytes_async(raw_path, img_bytes)
                _log(f"\u2705 [STEP 0/5] \ub2e4\uc6b4\ub85c\ub4dc \uc644\ub8cc | {len(img_bytes)/1024:.1f}KB | {time.time()-step_start:.2f}s")
                # await _async_archive() # [Restored comment]
            

                # 1) Gemini ë³´ì •
                step_start = time.time()
                _log("\U0001f4cc [STEP 1/5] Gemini \uc774\ubbf8\uc9c0 \ubcf4\uc815 \ubc0f \ud0dc\uadf8 \ucd94\ucd9c \uc2dc\uc791...")
                await _sse("gemini", "ëª…ì•”ê³¼ í˜•íƒœë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ë¸Œë¦­ ìƒ‰ìƒìœ¼ë¡œ ì˜®ê¸°ê¸° ì¢‹ì€ ìƒíƒœë¡œ ë³´ì •í•˜ê³  ìˆì–´ìš”.")
                corrected_bytes, ai_subject, ai_tags = await render_one_image_async(img_bytes, "image/png")

                final_subject = subject or ai_subject

                corrected_path = out_req_dir / "corrected.png"
                await _write_bytes_async(corrected_path, corrected_bytes)
                corrected_url = to_generated_url(corrected_path, out_dir=out_req_dir)
                _log(f"\u2705 [STEP 1/5] Gemini \uc644\ub8cc | Subject: {final_subject} | Tags: {ai_tags} | {time.time()-step_start:.2f}s")
                # await _async_archive() # [Restored comment]
                

                await update_job_suggested_tags(job_id, ai_tags)

                # 2) Tripo 3D
                step_start = time.time()
                _log(f"\U0001f4cc [STEP 2/4] Tripo 3D \ubaa8\ub378 \uc0dd\uc131 \uc2dc\uc791 (image-to-model)... (timeout={TRIPO_WAIT_TIMEOUT_SEC}s)")
                await _sse("tripo", "2D ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ 3D í˜•íƒœë¥¼ ì¡ì•„ë´…ë‹ˆë‹¤.")
                await update_job_stage(job_id, "THREE_D_PREVIEW")

                async with TripoClient(api_key=TRIPO_API_KEY) as client:
                    task_id = await client.image_to_model(image=str(corrected_path))
                    _log(f"   \U0001f504 Tripo \uc791\uc5c5 \uc0dd\uc131\ub428 | taskId={task_id}")

                    # [CHANGE] Custom Async Polling (Non-blocking)
                    # wait_for_task might use time.sleep(), blocking the event loop.
                    # We implement our own loop with asyncio.sleep() to allow _auto_flush_logs to run.
                    start_time = time.time()
                    last_progress = 0 # [FIX] Initialize variable
                    while True:
                        if time.time() - start_time > TRIPO_WAIT_TIMEOUT_SEC:
                            raise RuntimeError(f"Tripo task timed out after {TRIPO_WAIT_TIMEOUT_SEC}s")

                        # Check status
                        task = await client.get_task(task_id)
                        if task.status == TaskStatus.SUCCESS:
                            break
                        elif task.status in (TaskStatus.FAILED, TaskStatus.CANCELLED):
                             raise RuntimeError(f"Tripo task failed: status={task.status}")
                        
                        # [CHANGE] Only log if progress changes or every 10s (Heartbeat)
                        # This prevents spamming "Generating..." every 2 seconds.
                        current_progress = task.progress if hasattr(task, 'progress') else '?'
                        elapsed = time.time() - start_time
                        
                        # Log only on progress change or every 10s
                        if current_progress != last_progress or (int(elapsed) % 10 == 0 and int(elapsed) > 0):
                             _log(f"      [Tripo] Generating... ({int(elapsed)}s) | progress={current_progress}")
                             last_progress = current_progress

                        await asyncio.sleep(5.0) # Yield control to _auto_flush_logs

                    _log(f"   \u2705 Tripo \uc791\uc5c5 \uc644\ub8cc | status={task.status}")
                    downloaded = await client.download_task_models(task, str(out_tripo_dir))
                    _log(f"   \U0001f4e5 Tripo \ud30c\uc77c \ub2e4\uc6b4\ub85c\ub4dc \uc644\ub8cc | files={list(downloaded.keys()) if downloaded else 'None'}")

                tripo_elapsed = time.time() - step_start
                _log(f"\u2705 [STEP 2/4] Tripo \uc644\ub8cc | {tripo_elapsed:.2f}s")
                # await _async_archive() # [Restored comment]

                # 3-1) downloaded ì •ê·œí™”
                
                fixed_downloaded: Dict[str, str] = {}
                for model_type, path_or_url in (downloaded or {}).items():
                    if not path_or_url:
                        continue
                    s = str(path_or_url)
                    if s.startswith(("http://", "https://")):
                        ext_guess = ".glb" if ".glb" in s.lower() else ".bin"
                        dst = out_tripo_dir / f"{model_type}{ext_guess}"
                        await _download_http_to_file(s, dst)
                        fixed_downloaded[model_type] = str(dst)
                    else:
                        fixed_downloaded[model_type] = s

                missing = []
                for k, v in fixed_downloaded.items():
                    pv = Path(v)
                    if not pv.exists():
                        missing.append((k, v, "NOT_EXISTS"))
                    elif pv.stat().st_size == 0:
                        missing.append((k, v, "ZERO_SIZE"))
                if missing:
                    raise RuntimeError(f"Downloaded files missing: {missing}")

                # 3-2) URL map
                files_url: Dict[str, str] = {}
                for model_type, path_str in fixed_downloaded.items():
                    files_url[model_type] = to_generated_url(Path(path_str), out_dir=out_tripo_dir)

                if not any(u.lower().endswith(".glb") for u in files_url.values()):
                    glb_fallback = find_glb_in_dir(out_tripo_dir)
                    if glb_fallback:
                        files_url["glb"] = to_generated_url(glb_fallback, out_dir=out_tripo_dir)

                if not files_url:
                    raise RuntimeError("No downloadable model files found in out_tripo_dir")

                model_url = _pick_model_url(files_url)

                # 3) GLB í™•ë³´
                glb_path = pick_glb_from_downloaded(fixed_downloaded, out_tripo_dir)

                if glb_path is None:
                    local = _local_generated_path_from_url(model_url)
                    if local and local.exists() and local.stat().st_size > 0:
                        glb_path = local
                    else:
                        if not model_url.startswith(("http://", "https://")):
                            raise RuntimeError(f"Cannot resolve glb for brickify: {model_url}")
                        glb_path = out_brick_dir / "input.glb"
                        await _download_http_to_file(model_url, glb_path)

                if not glb_path.exists() or glb_path.stat().st_size == 0:
                    raise RuntimeError(f"GLB missing/empty: {glb_path}")

                _log(f"   \U0001f4e6 GLB \uc900\ube44\uc644\ub8cc | path={glb_path.name} | size={glb_path.stat().st_size/1024:.1f}KB")

                # 4) CoScientist Brickify (Generate â†’ Debate â†’ Evolve)
                step_start = time.time()
                eff_budget = int(budget) if budget is not None else int(AGE_TO_BUDGET.get(age.strip(), 100))
                # PRO ëª¨ë“œ(5000ê°œ ìˆ˜ì¤€) ì‹œ ë³µì…€ ì œí•œ ìƒí–¥ (í•´ìƒë„ ìœ ì§€)
                v_limit = 50000 if eff_budget >= 4000 else (20000 if eff_budget >= 1000 else 6000)
                start_target = budget_to_start_target(eff_budget)

                _log(f"ğŸš€ [STEP 3/4] CoScientist Brickify ì‹œì‘... | budget={eff_budget} | target={start_target}")
                await update_job_stage(job_id, "MODEL")
                await _sse("brickify", "ë¸Œë¦­ ë‹¨ìœ„ë¡œ ë¶„í•´í•˜ë©´ì„œ ì•ˆì •ì ì¸ ì¡°í•©ì„ íƒìƒ‰ ì¤‘ì´ì—ìš”.")

                out_ldr = out_brick_dir / "result.ldr"

                # Brickify íŒŒë¼ë¯¸í„° (regeneration_loop & fallback ê³µìš©)
                brickify_params = dict(
                    target=start_target,
                    budget=eff_budget,
                    min_target=5,
                    shrink=0.6,
                    search_iters=10,
                    kind="brick",
                    plates_per_voxel=3,
                    interlock=True,
                    max_area=20,
                    solid_color=4,
                    use_mesh_color=True,
                    invert_y=False,
                    smart_fix=True,
                    span=4,
                    max_new_voxels=v_limit,
                    refine_iters=4,
                    ensure_connected=True,
                    min_embed=2,
                    erosion_iters=1,
                    fast_search=True,
                    step_order="bottomup",
                    extend_catalog=True,
                    max_len=8,
                    avoid_1x1=True,
                )

                # CoScientist regeneration_loop ì‹œë„ â†’ ì‹¤íŒ¨ ì‹œ ë‹¨ìˆœ brickify fallback
                used_coscientist = False
                try:
                    regen_loop_fn, gemini_cls = load_agent_modules()
                    _log("ğŸ”¬ [CoScientist] LLM ì¬ìƒì„± ì—ì´ì „íŠ¸ í™œì„±í™”")
                    await _sse("coscientist", "CoScientistê°€ êµ¬ì¡°ë¥¼ ê²€ì¦í•˜ë©° ìµœì ì˜ ë¸Œë¦­ ë°°ì¹˜ë¥¼ ì°¾ê³  ìˆì–´ìš”.")

                    # SSE ë¡œê·¸ ì½œë°±ì„ paramsì— ì£¼ì…
                    regen_params = brickify_params.copy()
                    regen_params["log_callback"] = make_agent_log_sender(job_id)

                    def run_coscientist():
                        return regen_loop_fn(
                            glb_path=str(glb_path),
                            output_ldr_path=str(out_ldr),
                            subject_name=final_subject or "Unknown Object",
                            llm_client=gemini_cls(),
                            max_retries=1,
                            acceptable_failure_ratio=0.1,
                            params=regen_params,
                        )

                    final_state = await anyio.to_thread.run_sync(run_coscientist)

                    # ê²°ê³¼ ì¶”ì¶œ
                    report = final_state.get('final_report', {})
                    metrics = report.get('final_metrics', {})
                    parts_count = metrics.get('total_bricks', 0)
                    # LDR íŒŒì¼ì—ì„œ ì§ì ‘ ì¹´ìš´íŠ¸ (fallback)
                    if parts_count == 0 and out_ldr.exists():
                        ldr_text = out_ldr.read_text(encoding='utf-8')
                        parts_count = sum(1 for line in ldr_text.splitlines() if line.startswith('1 '))
                    result = {"parts": parts_count, "final_target": start_target}
                    used_coscientist = True
                    _log(f"ğŸ”¬ [CoScientist] ì™„ë£Œ | ì„±ê³µ={report.get('success', '?')} | ì‹œë„={report.get('total_attempts', '?')}íšŒ")

                except Exception as cos_err:
                    _log(f"âš ï¸ [CoScientist] ì‹¤íŒ¨, ë‹¨ìˆœ Brickifyë¡œ fallback: {cos_err}")

                    # Fallback: ê¸°ì¡´ ë‹¨ìˆœ brickify
                    global _CONVERT_FN
                    if _CONVERT_FN is None:
                        _CONVERT_FN = load_engine_convert()

                    def run_brickify():
                        return _CONVERT_FN(str(glb_path), str(out_ldr), **brickify_params)

                    result = await anyio.to_thread.run_sync(run_brickify)

                brickify_elapsed = time.time() - step_start
                engine_label = "CoScientist" if used_coscientist else "Brickify"
                _log(f"\u2705 [STEP 3/4] {engine_label} \uc644\ub8cc | parts={result.get('parts')} | target={result.get('final_target')} | {brickify_elapsed:.2f}s")
                # await _async_archive() # [Restored comment]
                

                if not out_ldr.exists() or out_ldr.stat().st_size == 0:
                    raise RuntimeError(f"LDR output missing/empty: {out_ldr}")

                # 5) ê²°ê³¼ URL + BOM
                step_start = time.time()
                s3_mode = "ON" if USE_S3 else "OFF"
                _log(f"\U0001f4cc [STEP 4/4] \uacb0\uacfc URL \uc0dd\uc131 \ubc0f BOM \ud30c\uc77c \uc0dd\uc131 \uc911... (S3={s3_mode})")
                await _sse("bom", "í˜„ì¬ ì„¤ê³„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•„ìš”í•œ ë¶€í’ˆ ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³  ìˆì–´ìš”.")
                ldr_url = to_generated_url(out_ldr, out_dir=out_brick_dir)

                _log("   \U0001f4cb BOM \ud30c\uc77c \uc0dd\uc131 \uc911...")
                bom_data = await anyio.to_thread.run_sync(generate_bom_from_ldr, out_ldr)
                out_bom = out_brick_dir / "bom.json"
                await _write_bytes_async(out_bom, json.dumps(bom_data, indent=2, ensure_ascii=False).encode("utf-8"))
                bom_url = to_generated_url(out_bom, out_dir=out_brick_dir)
                _log(f"   \u2705 BOM \ud30c\uc77c \uc0dd\uc131 \uc644\ub8cc | total_parts={bom_data['total_parts']} | unique={len(bom_data['parts'])}")

                _log(f"\u2705 [STEP 4/4] URL \uc0dd\uc131 \uc644\ub8cc | {time.time()-step_start:.2f}s")

                # 5-2) PDF + Screenshot ìƒì„± ìš”ì²­ (SQSë¡œ ìœ„ì„)
                pdf_url = None
                await _sse("pdf", "ì¡°ë¦½ ìˆœì„œë¥¼ ì •ë¦¬í•´ì„œ ì„¤ëª…ì„œë¡œ ì˜®ê¸°ê³  ìˆì–´ìš”.")
                try:
                    await send_pdf_request_message(
                        job_id=job_id,
                        ldr_url=ldr_url,
                        model_name=final_subject or "Brickers Model",
                    )
                    _log("ğŸ“¤ [STEP 5/5] PDF ìƒì„± ìš”ì²­ ì „ì†¡ (brickers-blueprints-queue)")
                except Exception as pdf_err:
                    _log(f"âš ï¸ [STEP 5/5] PDF SQS ì „ì†¡ ì‹¤íŒ¨ (íŒŒì´í”„ë¼ì¸ ê³„ì†): {pdf_err}")

                try:
                    await send_screenshot_request_message(
                        job_id=job_id,
                        ldr_url=ldr_url,
                        model_name=final_subject or "Brickers Model",
                    )
                    _log("ğŸ“¤ [STEP 5/5] ìŠ¤í¬ë¦°ìƒ· ìƒì„± ìš”ì²­ ì „ì†¡ (brickers-screenshots-queue)")
                except Exception as ss_err:
                    _log(f"âš ï¸ [STEP 5/5] ìŠ¤í¬ë¦°ìƒ· SQS ì „ì†¡ ì‹¤íŒ¨ (íŒŒì´í”„ë¼ì¸ ê³„ì†): {ss_err}")

                await _sse("complete", "ì„¤ê³„ê°€ ëë‚¬ì–´ìš”. ê²°ê³¼ë¥¼ í•œë²ˆ ì‚´í´ë³¼ê¹Œìš”?")

                total_elapsed = time.time() - total_start
                _log("\u2550" * 70)
                _log(f"\U0001f389 [AI-SERVER] \uc694\uccad \uc644\ub8cc! | jobId={job_id}")
                _log(f"\u23f1\ufe0f  \ucd1d \uc18c\uc694\uc2dc\uac04: {total_elapsed:.2f}s ({total_elapsed/60:.1f}\ubd84)")
                _log(f"   - Tripo 3D: {tripo_elapsed:.2f}s")
                _log(f"   - Brickify: {brickify_elapsed:.2f}s")
                _log(f"\U0001f4e6 \uacb0\uacfc: parts={result.get('parts')} | ldrSize={out_ldr.stat().st_size/1024:.1f}KB")
                _log("\u2550" * 70)
            

            # [Restored comment]
            # try:
            #     await archive_job_logs(job_id, list(job_log_buffer), status="SUCCESS")
            # except:
            #     pass

            return {
                "correctedUrl": corrected_url,
                "modelUrl": model_url,
                "ldrUrl": ldr_url,
                "bomUrl": bom_url,
                "pdfUrl": pdf_url,
                "subject": final_subject,
                "tags": ai_tags,
                "parts": int(result.get("parts", 0)),
                "finalTarget": int(result.get("final_target", 0)),
            }

    except Exception as e:
        total_elapsed = time.time() - total_start
        tb = traceback.format_exc()
        
        # LogCaptureê°€ ì´ë¯¸ Exception ì¶œë ¥ë„ ì¡ì•˜ì„ ìˆ˜ ìˆì§€ë§Œ,
        # ëª…ì‹œì ìœ¼ë¡œ ì°ì–´ì£¼ëŠ” ê²ƒì´ ì•ˆì „
        _log("\u2550" * 70)
        _log(f"\u274c [AI-SERVER] \uc694\uccad \uc2e4\ud328! | jobId={job_id} | \uc18c\uc694\uc2dc\uac04={total_elapsed:.2f}s")
        _log(f"\u274c \uc5d0\ub7ec: {str(e)}")
        _log("\u2550" * 70)
        log(tb, user_email=user_email) # ì´ê²ƒë„ ìº¡ì³ë¨
        
        _write_error_log(out_req_dir, tb)
        _write_error_log(out_tripo_dir, tb)
        _write_error_log(out_brick_dir, tb)
        

        # [Restored comment]
        # try:
        #     await archive_job_logs(job_id, list(job_log_buffer), status="FAILED")
        # except:
        #     pass

        raise RuntimeError(str(e)) from e


# --------------- HTTP endpoint ---------------

@router.post("/process-all", response_model=ProcessResp)
async def process(request: KidsProcessRequest):
    """Kids Mode ì²˜ë¦¬ (HTTP ì—”ë“œí¬ì¸íŠ¸) - í˜¸í™˜ì„± ìœ ì§€"""
    req_id = uuid.uuid4().hex

    try:
        result = await process_kids_request_internal(
            job_id=req_id,
            source_image_url=request.sourceImageUrl,
            age=request.age,
            budget=request.budget,
            subject=request.subject,
            user_email=request.userEmail or "unknown",
        )

        ldr_data_uri: Optional[str] = None
        if not USE_S3 and request.returnLdrData:
            ldr_path = _local_generated_path_from_url(result["ldrUrl"])
            if ldr_path and ldr_path.exists():
                b = await _read_bytes_async(ldr_path)
                b64_str = base64.b64encode(b).decode("utf-8")
                ldr_data_uri = f"data:text/plain;base64,{b64_str}"

        return {
            "ok": True,
            "reqId": req_id,
            "correctedUrl": result["correctedUrl"],
            "taskId": req_id,
            "modelUrl": result["modelUrl"],
            "files": {},
            "ldrUrl": result["ldrUrl"],
            "ldrData": ldr_data_uri,
            "bomUrl": result["bomUrl"],
            "pdfUrl": result.get("pdfUrl"), # [New]
            "subject": result["subject"],
            "tags": result["tags"],
            "parts": result["parts"],
            "finalTarget": result["finalTarget"],
        }

    except HTTPException:
        raise
    except Exception as e:
        if DEBUG:
            raise HTTPException(status_code=500, detail={"reqId": req_id, "error": str(e)})
        raise HTTPException(status_code=500, detail="process failed")
