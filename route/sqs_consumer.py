"""
SQS Consumer - AI Serverì—ì„œ Backendë¡œë¶€í„° REQUEST ë©”ì‹œì§€ ìˆ˜ì‹ 

ì•„í‚¤í…ì²˜: SQS Poller â†’ asyncio.Queue â†’ ë‹¨ì¼ Worker (ìˆœì°¨ ì²˜ë¦¬)
- í´ë§ì€ ë…ë¦½ì ìœ¼ë¡œ ê³„ì† ì§„í–‰ (ì´ë²¤íŠ¸ë£¨í”„ non-blocking)
- ì‘ì—…ì€ ë‚´ë¶€ íì— ìŒ“ì˜€ë‹¤ê°€ 1ê°œì”© ìˆœì°¨ ì²˜ë¦¬
- ë¡œê·¸ ì„ì„ ì—†ìŒ, ë¦¬ì†ŒìŠ¤ ê²½ìŸ ì—†ìŒ
"""
from __future__ import annotations

import os
import json
import asyncio
import anyio
from datetime import datetime
from typing import Dict, Any

from route.kids_render import process_kids_request_internal
from route.sqs_producer import send_result_message
from service.backend_client import check_job_canceled
from service.log_context import JobLogContext
from brick_engine.agent.log_analyzer.persistence import archive_job_logs


def log(msg: str, user_email: str = "System") -> None:
    """íƒ€ì„ìŠ¤íƒ¬í”„ ë° ì‚¬ìš©ì ì •ë³´ í¬í•¨ ë¡œê·¸ ì¶œë ¥"""
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
    user_tag = f"[{user_email}]" if user_email else "[System]"
    print(f"[{ts}] {user_tag} {msg}", flush=True)


def _is_truthy(v: str) -> bool:
    return v.strip().lower() in ("1", "true", "yes", "y", "on")


# í™˜ê²½ ë³€ìˆ˜
AWS_REGION = os.environ.get("AWS_REGION", "ap-northeast-2").strip()
SQS_REQUEST_QUEUE_URL = os.environ.get("AWS_SQS_REQUEST_QUEUE_URL", "").strip()
SQS_ENABLED = _is_truthy(os.environ.get("AWS_SQS_ENABLED", "false"))
SQS_POLL_INTERVAL = int(os.environ.get("SQS_POLL_INTERVAL", "5"))
SQS_MAX_MESSAGES = int(os.environ.get("SQS_MAX_MESSAGES", "1"))  # ìˆœì°¨ ì²˜ë¦¬ì´ë¯€ë¡œ 1ê°œì”©
SQS_WAIT_TIME = int(os.environ.get("SQS_WAIT_TIME", "10"))
SQS_VISIBILITY_TIMEOUT = int(os.environ.get("SQS_VISIBILITY_TIMEOUT", "1800"))

# ì „ì—­ ìƒíƒœ (ëª¨ë‹ˆí„°ë§ìš©)
_TOTAL_REQUESTS_RECEIVED = 0
_TOTAL_REQUESTS_COMPLETED = 0
_TOTAL_REQUESTS_FAILED = 0

# ë‚´ë¶€ ì‘ì—… í (Lazy Init)
_JOB_QUEUE: asyncio.Queue | None = None


def _get_job_queue() -> asyncio.Queue:
    global _JOB_QUEUE
    if _JOB_QUEUE is None:
        _JOB_QUEUE = asyncio.Queue(maxsize=100)
    return _JOB_QUEUE


# boto3 lazy import
try:
    import boto3
    from botocore.exceptions import ClientError
except ImportError:
    boto3 = None  # type: ignore
    ClientError = Exception  # type: ignore


_SQS_CLIENT = None


def _get_sqs_client():
    """SQS Client ì‹±ê¸€í†¤"""
    global _SQS_CLIENT
    if _SQS_CLIENT is not None:
        return _SQS_CLIENT

    if not SQS_ENABLED:
        raise RuntimeError("SQS is not enabled (AWS_SQS_ENABLED=false)")

    if boto3 is None:
        raise RuntimeError("boto3 is not installed (pip install boto3)")

    if not SQS_REQUEST_QUEUE_URL:
        raise RuntimeError("AWS_SQS_REQUEST_QUEUE_URL is not set")

    _SQS_CLIENT = boto3.client("sqs", region_name=AWS_REGION)
    return _SQS_CLIENT


# ============================================================================
# SQS Poller â€” íì—ì„œ ë©”ì‹œì§€ë¥¼ ê°€ì ¸ì™€ ë‚´ë¶€ íì— ë„£ê¸°ë§Œ í•¨
# ============================================================================

_POLL_COUNT = 0


async def _sqs_poller():
    """
    SQS í´ë§ ë£¨í”„ (ë…ë¦½ ì½”ë£¨í‹´)
    - Long pollingìœ¼ë¡œ ë©”ì‹œì§€ ìˆ˜ì‹ 
    - ë‚´ë¶€ asyncio.Queueì— ë„£ê¸°ë§Œ í•¨ (ì²˜ë¦¬ëŠ” Workerê°€ ë‹´ë‹¹)
    - ì²˜ë¦¬ ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦¬ì§€ ì•Šìœ¼ë¯€ë¡œ ì´ë²¤íŠ¸ë£¨í”„ blocking ì—†ìŒ
    """
    global _POLL_COUNT

    job_queue = _get_job_queue()

    while True:
        try:
            sqs = _get_sqs_client()
            _POLL_COUNT += 1

            if _POLL_COUNT % 10 == 1:
                q_size = job_queue.qsize()
                log(f"ğŸ”„ [Poller] í´ë§ ì¤‘... (poll #{_POLL_COUNT}) | ëŒ€ê¸° í: {q_size}ê°œ")

            # boto3ëŠ” blockingì´ë¯€ë¡œ ì“°ë ˆë“œì—ì„œ ì‹¤í–‰
            def _receive():
                return sqs.receive_message(
                    QueueUrl=SQS_REQUEST_QUEUE_URL,
                    MaxNumberOfMessages=SQS_MAX_MESSAGES,
                    WaitTimeSeconds=SQS_WAIT_TIME,
                    VisibilityTimeout=SQS_VISIBILITY_TIMEOUT,
                )

            response = await anyio.to_thread.run_sync(_receive)
            messages = response.get("Messages", [])

            if messages:
                global _TOTAL_REQUESTS_RECEIVED
                for m in messages:
                    _TOTAL_REQUESTS_RECEIVED += 1
                    await job_queue.put((m, _TOTAL_REQUESTS_RECEIVED))
                log(
                    f"ğŸ“¥ [Poller] ë©”ì‹œì§€ ìˆ˜ì‹  â†’ ë‚´ë¶€ í ì „ë‹¬ | "
                    f"count={len(messages)} | í ëŒ€ê¸°: {job_queue.qsize()}ê°œ"
                )
                # ë©”ì‹œì§€ ìˆìœ¼ë©´ ì¦‰ì‹œ ë‹¤ìŒ í´ë§
                await asyncio.sleep(0.1)
            else:
                await asyncio.sleep(SQS_POLL_INTERVAL)

        except Exception as e:
            log(f"âŒ [Poller] í´ë§ ì‹¤íŒ¨ | poll #{_POLL_COUNT} | error={str(e)}")
            await asyncio.sleep(SQS_POLL_INTERVAL)


# ============================================================================
# Job Worker â€” ë‚´ë¶€ íì—ì„œ í•˜ë‚˜ì”© êº¼ë‚´ ìˆœì°¨ ì²˜ë¦¬
# ============================================================================

async def _job_worker():
    """
    ë‹¨ì¼ ì›Œì»¤ ì½”ë£¨í‹´ (ìˆœì°¨ ì²˜ë¦¬)
    - ë‚´ë¶€ íì—ì„œ ë©”ì‹œì§€ë¥¼ 1ê°œì”© êº¼ë‚´ì„œ ì²˜ë¦¬
    - ë™ì‹œ ì‹¤í–‰ ì—†ìŒ â†’ ë¡œê·¸ ì„ì„ ì—†ìŒ, ë¦¬ì†ŒìŠ¤ ê²½ìŸ ì—†ìŒ
    - ê° ì‘ì—… ì™„ë£Œ í›„ ë‹¤ìŒ ì‘ì—… ì‹œì‘
    """
    global _TOTAL_REQUESTS_COMPLETED, _TOTAL_REQUESTS_FAILED

    job_queue = _get_job_queue()
    log("[Worker] ğŸ­ ë‹¨ì¼ ì›Œì»¤ ì‹œì‘ (ìˆœì°¨ ì²˜ë¦¬ ëª¨ë“œ)")

    while True:
        try:
            # íì—ì„œ ë‹¤ìŒ ì‘ì—… ëŒ€ê¸° (blocking but async-safe)
            message, request_num = await job_queue.get()

            message_id = message.get("MessageId", "unknown")
            receipt_handle = message["ReceiptHandle"]

            log("=" * 60)
            log(f"ğŸ“¨ [Worker] ì‘ì—… ì‹œì‘ | #{request_num} | messageId={message_id}")
            log(f"   ğŸ“‹ ë‚¨ì€ ëŒ€ê¸°: {job_queue.qsize()}ê°œ")

            try:
                body = json.loads(message["Body"])
                user_email = body.get("userEmail", "unknown")
                job_id = body.get("jobId", "unknown")

                # [NEW] Job Log Context Start (Capture logs from here)
                job_log_buffer = []
                with JobLogContext(job_log_buffer):
                    log(f"ğŸ“¨ [Worker] ì²˜ë¦¬ ì‹œì‘ | jobId={job_id}", user_email=user_email)
                    log(f"   - type: {body.get('type')}", user_email=user_email)

                    # REQUEST íƒ€ì…ë§Œ ì²˜ë¦¬
                    if body.get("type") != "REQUEST":
                        log(f"âš ï¸ [Worker] RESULT íƒ€ì… ë¬´ì‹œ", user_email=user_email)
                        delete_message(receipt_handle)
                        job_queue.task_done()
                        continue

                    source_image_url = body.get("sourceImageUrl")
                    age = body.get("age", "6-7")
                    budget = body.get("budget")
                    language = body.get("language", "en") # [NEW]

                    # ì·¨ì†Œ ì—¬ë¶€ í™•ì¸ (ì²˜ë¦¬ ì‹œì‘ ì „)
                    if await check_job_canceled(job_id):
                        log(f"ğŸš« [Worker] ì·¨ì†Œëœ ì‘ì—… ìŠ¤í‚µ | jobId={job_id}", user_email=user_email)
                        delete_message(receipt_handle)
                        job_queue.task_done()
                        await archive_job_logs(job_id, list(job_log_buffer), status="CANCELED") # [NEW]
                        continue


                # [Legacy: Auto-Flush Logic - Commented out as per user request]
                # async def _auto_flush_logs():
                #     """
                #     ì£¼ê¸°ì ìœ¼ë¡œ ë¡œê·¸ ë²„í¼ë¥¼ DBì— ì „ì†¡ (Real-time UX)
                #     Tripo ìƒì„± ë“± ê¸´ ì‘ì—… ì¤‘ì—ë„ ì‚¬ìš©ìê°€ ë¡œê·¸ë¥¼ ë³¼ ìˆ˜ ìˆê²Œ í•¨.
                #     """
                #     last_sent_count = 0
                #     while True:
                #         await asyncio.sleep(5.0) # 5ì´ˆë§ˆë‹¤ ì²´í¬ (ë¶€í•˜ ê°ì†Œ)
                #         current_count = len(job_log_buffer)
                #         if current_count > last_sent_count:
                #             # ë³€ê²½ì‚¬í•­ì´ ìˆì„ ë•Œë§Œ ì „ì†¡
                #             await archive_job_logs(job_id, list(job_log_buffer), status="RUNNING")
                #             last_sent_count = current_count
                #
                # # Start Auto-Flush Task
                # flusher_task = asyncio.create_task(_auto_flush_logs())

                # Kids ë Œë”ë§ ì‹¤í–‰ (ìˆœì°¨ â€” ì´ ì‘ì—…ì´ ëë‚˜ì•¼ ë‹¤ìŒ ì‘ì—… ì‹œì‘)
                # [CHANGE] Pass external buffer
                result = await process_kids_request_internal(
                    job_id=job_id,
                    source_image_url=source_image_url,
                    age=age,
                    budget=budget,
                    user_email=user_email,
                    language=language, # [NEW]
                    external_log_buffer=job_log_buffer, # [NEW]
                )

                log(f"âœ… AI ë Œë”ë§ ì™„ë£Œ!", user_email=user_email)
                log(f"   - correctedUrl: {result.get('correctedUrl', '')[:60]}...")
                log(f"   - modelUrl: {result.get('modelUrl', '')[:60]}...")
                log(f"   - ldrUrl: {result.get('ldrUrl', '')[:60]}...")
                log(f"   - parts: {result.get('parts')}, finalTarget: {result.get('finalTarget')}", user_email=user_email)

                # RESULT ë©”ì‹œì§€ ì „ì†¡ (ì„±ê³µ)
                log("ğŸ“¤ [Worker] RESULT ë©”ì‹œì§€ ì „ì†¡ ì¤‘...")
                await send_result_message(
                    job_id=job_id,
                    success=True,
                    corrected_url=result["correctedUrl"],
                    glb_url=result["modelUrl"],
                    ldr_url=result["ldrUrl"],
                    bom_url=result["bomUrl"],
                    pdf_url=result.get("pdfUrl", ""),
                    parts=result["parts"],
                    final_target=result["finalTarget"],
                    tags=result.get("tags", []),
                    background_url=result.get("backgroundUrl", ""),
                )

                delete_message(receipt_handle)
                _TOTAL_REQUESTS_COMPLETED += 1

                log(f"âœ… [Worker] ì²˜ë¦¬ ì™„ë£Œ | jobId={job_id} | "
                    f"ì™„ë£Œ: {_TOTAL_REQUESTS_COMPLETED} | ì‹¤íŒ¨: {_TOTAL_REQUESTS_FAILED}",
                    user_email=user_email)
                log("=" * 60, user_email=user_email)
                
                # [NEW] Archive Final State (Success)
                await archive_job_logs(job_id, list(job_log_buffer), status="SUCCESS")

                # finally:
                #     # ì‘ì—… ì¢…ë£Œ ì‹œ í”ŒëŸ¬ì…” ì •ë¦¬
                #     flusher_task.cancel()
                #     try:
                #         await flusher_task
                #     except asyncio.CancelledError:
                #         pass


            except json.JSONDecodeError as e:
                log(f"âŒ [Worker] JSON íŒŒì‹± ì‹¤íŒ¨ | messageId={message_id} | error={str(e)}")
                delete_message(receipt_handle)
                _TOTAL_REQUESTS_FAILED += 1

            except Exception as e:
                u_email = locals().get("user_email", "unknown")
                job_id = locals().get("job_id", "unknown")
                
                # Check if context was active? 
                # If exception happened inside context, logs are in buffer.
                # If before context, buffer might not exist.
                # But here buffer init is very early.
                
                # If job_log_buffer exists in locals():
                j_buf = locals().get("job_log_buffer", [])
                
                # Re-enter context if needed? No, context exited on exception.
                # But we can still use buffer.
                
                # Temporarily re-hook logging to buffer for error logging??
                # Or just append manually?
                # Actually GlobalLogCapture hooks ALL stdout/stderr.
                # But since we exited the context block (due to exception), 
                # future logs won't go to buffer unless we re-enter.
                
                # Let's re-enter context for error logging if buffer exists
                if 'job_log_buffer' in locals():
                    with JobLogContext(j_buf):
                        log(f"âŒ [Worker] ì²˜ë¦¬ ì‹¤íŒ¨ | error={str(e)}", user_email=u_email)

                        # RESULT ë©”ì‹œì§€ ì „ì†¡ (ì‹¤íŒ¨)
                        try:
                            # job_id is already set
                            await send_result_message(
                                job_id=job_id,
                                success=False,
                                error_message=str(e),
                            )
                        except Exception as send_error:
                            log(f"âŒ [Worker] ê²°ê³¼ ì „ì†¡ ì‹¤íŒ¨ | error={str(send_error)}", user_email=u_email)

                        delete_message(receipt_handle)
                        _TOTAL_REQUESTS_FAILED += 1
                        
                        # [NEW] Archive Final State (Failed)
                        await archive_job_logs(job_id, list(j_buf), status="FAILED")
                else:
                    # Fallback system logging
                    log(f"âŒ [Worker] ì²˜ë¦¬ ì‹¤íŒ¨ (No Context) | error={str(e)}", user_email=u_email)
                    delete_message(receipt_handle)
                    _TOTAL_REQUESTS_FAILED += 1

            finally:
                job_queue.task_done()

        except Exception as e:
            log(f"âŒ [Worker] ì›Œì»¤ ì˜ˆì™¸ | error={str(e)}")
            await asyncio.sleep(1)


# ============================================================================
# Helpers
# ============================================================================

def delete_message(receipt_handle: str):
    """ë©”ì‹œì§€ ì‚­ì œ"""
    try:
        sqs = _get_sqs_client()
        sqs.delete_message(
            QueueUrl=SQS_REQUEST_QUEUE_URL,
            ReceiptHandle=receipt_handle,
        )
    except Exception as e:
        log(f"âŒ [SQS] ë©”ì‹œì§€ ì‚­ì œ ì‹¤íŒ¨ | error={str(e)}")


def get_consumer_stats() -> Dict[str, Any]:
    """ëª¨ë‹ˆí„°ë§ìš© í†µê³„ (admin ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ì‚¬ìš© ê°€ëŠ¥)"""
    q = _get_job_queue()
    return {
        "total_received": _TOTAL_REQUESTS_RECEIVED,
        "total_completed": _TOTAL_REQUESTS_COMPLETED,
        "total_failed": _TOTAL_REQUESTS_FAILED,
        "queue_size": q.qsize(),
        "poll_count": _POLL_COUNT,
    }


# ============================================================================
# Entry Point â€” FastAPI startupì—ì„œ í˜¸ì¶œ
# ============================================================================

async def start_consumer():
    """
    SQS Consumer ì‹œì‘ (2ê°œ ì½”ë£¨í‹´ ë™ì‹œ ì‹¤í–‰)
    1. _sqs_poller: SQS í´ë§ â†’ ë‚´ë¶€ íì— ë„£ê¸°
    2. _job_worker: ë‚´ë¶€ íì—ì„œ 1ê°œì”© êº¼ë‚´ ìˆœì°¨ ì²˜ë¦¬
    """
    if not SQS_ENABLED:
        log("[SQS Consumer] âš ï¸ SQS ë¹„í™œì„±í™” ìƒíƒœ (AWS_SQS_ENABLED=false)")
        return

    log("â•" * 70)
    log("[SQS Consumer] ğŸš€ ì‹œì‘ (Poller + ë‹¨ì¼ Worker ì•„í‚¤í…ì²˜)")
    log(f"   - Queue URL: {SQS_REQUEST_QUEUE_URL}")
    log(f"   - Poll Interval: {SQS_POLL_INTERVAL}ì´ˆ")
    log(f"   - Max Messages per poll: {SQS_MAX_MESSAGES}")
    log(f"   - Wait Time: {SQS_WAIT_TIME}ì´ˆ (Long polling)")
    log(f"   - Visibility Timeout: {SQS_VISIBILITY_TIMEOUT}ì´ˆ")
    log(f"   - Worker: ë‹¨ì¼ ìˆœì°¨ ì²˜ë¦¬ (ë™ì‹œ ì‹¤í–‰ ì—†ìŒ)")
    log("â•" * 70)

    # ë‘ ì½”ë£¨í‹´ì„ ë™ì‹œì— ì‹¤í–‰
    # Poller: SQS â†’ ë‚´ë¶€ í
    # Worker: ë‚´ë¶€ í â†’ ìˆœì°¨ ì²˜ë¦¬
    asyncio.create_task(_sqs_poller())
    asyncio.create_task(_job_worker())
