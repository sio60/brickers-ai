import os
import sys
import boto3
import tempfile
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from agent.memory_utils import memory_manager
from physical_verification.ldr_loader import LdrLoader
from physical_verification.pybullet_verifier import PyBulletVerifier

def analyze_ldr_file(ldr_path: Path):
    """LDR íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ë¬¼ë¦¬ ë©”íŠ¸ë¦­ ì¶”ì¶œ"""
    loader = LdrLoader()
    plan = loader.load_from_file(str(ldr_path))
    
    # 1. ê¸°ë³¸ ë©”íŠ¸ë¦­
    verifier = PyBulletVerifier(plan, gui=False)
    # ì¶©ëŒ ë° ì•ˆì •ì„± ê²€ì‚¬ (ë¹ ë¥¸ ë¶„ì„ì„ ìœ„í•´ ì§§ê²Œ ì§„í–‰)
    col_result = verifier.run_collision_check()
    stab_result = verifier.run_stability_check(duration=1.0)
    
    # 2. ìƒì„¸ ë¬¼ë¦¬ ë©”íŠ¸ë¦­ ê³„ì‚° (MemoryUtilsì˜ í—¬í¼ ì‚¬ìš©)
    phys_metrics = memory_manager.calculate_model_metrics(plan, stab_result)
    
    return {
        "plan": plan,
        "col_result": col_result,
        "stab_result": stab_result,
        "phys_metrics": phys_metrics
    }

def process_s3_ldr(bucket: str, key: str, session_id: str = "batch_import"):
    """S3ì˜ ë‹¨ì¼ LDR íŒŒì¼ì„ ë¡œì»¬ë¡œ ë°›ì•„ ë¶„ì„ í›„ DB ì €ì¥"""
    s3 = boto3.client('s3')
    
    with tempfile.NamedTemporaryFile(suffix=".ldr", delete=False) as tmp:
        tmp_path = Path(tmp.name)
        try:
            print(f"ğŸ“¥ Downloading s3://{bucket}/{key}...")
            s3.download_file(bucket, key, str(tmp_path))
            
            # ë¶„ì„ ì‹¤í–‰
            analysis = analyze_ldr_file(tmp_path)
            metrics = analysis["phys_metrics"]
            
            # DB ì €ì¥ (RAG ì‚¬ë¡€ë¡œ ë“±ë¡)
            print("ğŸ’¾ Logging to RAG DB...")
            memory_manager.log_experiment(
                session_id=session_id,
                model_id=Path(key).name,
                agent_type="historical_data",
                iteration=0,
                hypothesis={"observation": f"Historical data from S3: {key}", "hypothesis": "Recovered from S3"},
                experiment={"tool": "Original Generation", "params": {}},
                verification={
                    "passed": analysis["stab_result"].is_valid,
                    "metrics_after": metrics,
                    "numerical_analysis": f"Volume={metrics['total_volume']:.1f}, Bricks={metrics['total_bricks']}, Stability={analysis['stab_result'].stability_grade}"
                },
                improvement={"lesson_learned": "Historical case imported from S3"},
                async_save=False
            )
            print(f"âœ… Processed {key}")
            
        finally:
            if tmp_path.exists():
                tmp_path.unlink()

def main():
    # ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ì§ì ‘ ì…ë ¥)
    BUCKET = os.getenv("AWS_S3_BUCKET", "bricker-uploads")
    PREFIX = os.getenv("S3_PREFIX", "uploads/ai-generated")
    
    print(f"ğŸš€ Starting S3 LDR Analysis Batch | Bucket: {BUCKET}")
    
    s3 = boto3.client('s3')
    paginator = s3.get_paginator('list_objects_v2')
    
    for page in paginator.paginate(Bucket=BUCKET, Prefix=PREFIX):
        for obj in page.get('Contents', []):
            key = obj['Key']
            if key.endswith('.ldr'):
                try:
                    process_s3_ldr(BUCKET, key)
                except Exception as e:
                    print(f"âŒ Failed to process {key}: {e}")

if __name__ == "__main__":
    if not os.getenv("AWS_ACCESS_KEY_ID"):
        print("âš ï¸ AWS credentials not found. Please set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.")
    else:
        main()
