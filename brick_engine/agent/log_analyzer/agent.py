import json
import logging
import re
import docker
import asyncio
from typing import List, Optional, Dict, Any, Union
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
from langgraph.graph import StateGraph, END

from .state import LogAnalysisState
from .persistence import get_archived_logs
from ..llm_clients import GeminiClient
from ..agent_tools import (
    execute_read_file, 
    execute_check_db, 
    execute_check_system, 
    execute_check_sqs
)

# Logger configuration
logger = logging.getLogger("agent.log_agent.agent")

# --- Nodes (ë…¸ë“œ ì •ì˜) ---

async def fetch_logs_node(state: LogAnalysisState):
    """
    [ë…¸ë“œ 1: ë¡œê·¸ ìˆ˜ì§‘]
    Docker SDKì—ì„œ ì‹¤ì‹œê°„ ë¡œê·¸ë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜, ì—†ìœ¼ë©´ DB(Persistence)ì—ì„œ ë¡œê·¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    container_name = state.get("container_name", "brickers-ai-container")
    target_job_id = state.get("job_id")
    
    logger.info(f"--- [ë¡œê·¸ ë¶„ì„ê¸°] 1ë‹¨ê³„: ë¡œê·¸ ìˆ˜ì§‘ ì‹œì‘ (Job: {target_job_id or 'Auto'}) ---")
    
    raw_logs = ""
    
    # 1. ë¨¼ì € Dockerì—ì„œ ì‹¤ì‹œê°„ ë¡œê·¸ ì‹œë„
    try:
        client = docker.from_env()
        container = client.containers.get(container_name)
        raw_logs = container.logs(tail=2000).decode("utf-8", errors="replace")
        logger.info(f"âœ… Docker ì‹¤ì‹œê°„ ë¡œê·¸ ìˆ˜ì§‘ ì„±ê³µ ({len(raw_logs)} bytes)")
    except Exception as e:
        logger.warning(f"âš ï¸ Docker ë¡œê·¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        # Docker ì‹¤íŒ¨ ì‹œ stateì— ì´ë¯¸ ë¡œê·¸ê°€ ìˆëŠ”ì§€ í™•ì¸ (í…ŒìŠ¤íŠ¸ìš© ë“±)
        raw_logs = state.get("logs", "")

    # 2. Job ID ì¶”ì¶œ ë° ë¡œê·¸ í•„í„°ë§
    if not target_job_id:
        # ìµœê·¼ ì‹¤íŒ¨í•œ Job ê²€ìƒ‰
        failure_matches = re.findall(r"ìš”ì²­ ì‹¤íŒ¨! \| jobId=([a-f0-9-]+)", raw_logs)
        if failure_matches:
            target_job_id = failure_matches[-1]
            logger.info(f"ğŸ•µï¸ ìµœê·¼ ì‹¤íŒ¨í•œ Job ë°œê²¬: {target_job_id}")
        else:
            start_matches = re.findall(r"ìš”ì²­ ì‹œì‘ \| jobId=([a-f0-9-]+)", raw_logs)
            if start_matches:
                target_job_id = start_matches[-1]
                logger.info(f"â„¹ï¸ ìµœê·¼ ì‹œì‘ëœ Job ë°œê²¬: {target_job_id}")

    # 3. í•„í„°ë§ ë° DB Fallback
    filtered_logs = ""
    if target_job_id:
        job_logs_list = [line for line in raw_logs.splitlines() if target_job_id in line]
        
        if len(job_logs_list) < 5: # ë¡œê·¸ê°€ ë„ˆë¬´ ì ìœ¼ë©´ DB ì•„ì¹´ì´ë¸Œ í™•ì¸
            logger.info(f"ğŸ” ì‹¤ì‹œê°„ ë¡œê·¸ì— [{target_job_id}] ì •ë³´ê°€ ë¶€ì¡±í•¨. DB ì•„ì¹´ì´ë¸Œ ì¡°íšŒ ì¤‘...")
            archived = await get_archived_logs(target_job_id)
            if archived:
                filtered_logs = archived
                logger.info(f"âœ… DBì—ì„œ ì•„ì¹´ì´ë¸Œëœ ë¡œê·¸ ë¡œë“œ ì„±ê³µ ({len(filtered_logs.splitlines())}ì¤„)")
            else:
                filtered_logs = "\n".join(job_logs_list)
        else:
            filtered_logs = "\n".join(job_logs_list)
            logger.info(f"ğŸ“‚ ì‹¤ì‹œê°„ ë¡œê·¸ì—ì„œ [{target_job_id}] ê´€ë ¨ ë¡œê·¸ {len(job_logs_list)}ì¤„ í•„í„°ë§ ì™„ë£Œ.")
    else:
        filtered_logs = raw_logs[-4000:]
        logger.warning("âš ï¸ Job IDë¥¼ ì‹ë³„í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ 4000ìë§Œ ë¶„ì„í•©ë‹ˆë‹¤.")

    user_prompt = f"""
    [ëŒ€ìƒ Job ID: {target_job_id or "ì•Œ ìˆ˜ ì—†ìŒ"}]
    
    [ë¡œê·¸ ë°ì´í„°]
    {filtered_logs}
    
    ì´ ë¡œê·¸ë¥¼ ì •ë°€ ë¶„ì„í•˜ì—¬ ë¬¸ì œì˜ ê·¼ë³¸ ì›ì¸ì„ ì°¾ìœ¼ì‹­ì‹œì˜¤. 
    í•„ìš”í•˜ë‹¤ë©´ ì œê³µëœ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì½”ë“œë‚˜ ì‹œìŠ¤í…œ ìƒíƒœë¥¼ ì¶”ê°€ë¡œ í™•ì¸í•˜ì‹­ì‹œì˜¤.
    """
    
    return {
        "logs": filtered_logs,
        "messages": [HumanMessage(content=user_prompt)],
        "iteration": 0,
        "job_id": target_job_id
    }

async def diagnose_node(state: LogAnalysisState):
    """
    [ë…¸ë“œ 2: ì •ë°€ ì§„ë‹¨]
    """
    messages = state.get("messages", [])
    iteration = state.get("iteration", 0)
    
    system_prompt = """
    ë‹¹ì‹ ì€ ê³ ê¸‰ ì‹œìŠ¤í…œ ë””ë²„ê¹… ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.
    ë¶„ì„ ê²°ê³¼ëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ìš”ì•½í•˜ì—¬ ì œê³µí•˜ì‹­ì‹œì˜¤.
    
    1. ì›ì¸ íŒŒì•…: Traceback, DB ì—°ê²°, SQS ìƒíƒœ ë“±ì„ ì¢…í•© ë¶„ì„.
    2. ë„êµ¬ í™œìš©: êµ¬ì²´ì ì¸ íŒŒì¼ ë‚´ìš© í™•ì¸ì´ë‚˜ ìƒíƒœ ì¡°íšŒê°€ í•„ìš”í•˜ë©´ ë„êµ¬ë¥¼ í˜¸ì¶œ.
    3. í•´ê²°ì±… ì œì•ˆ: ë‹¨ìˆœíˆ ì—ëŸ¬ë©”ì‹œì§€ë¥¼ ì½ëŠ” ê²Œ ì•„ë‹ˆë¼, ì•„í‚¤í…ì²˜ ê°œì„ ì•ˆì´ë‚˜ ì½”ë“œ ìˆ˜ì •ì•ˆì„ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œ.
    
    ì¶œë ¥ í˜•ì‹ (JSON):
    - ë„êµ¬ ì‚¬ìš©: {"action": "read_file", "args": {"file_path": "..."}, "reasoning": "..."}
    - ì™„ë£Œ: {"action": "finish", "analysis": {"error_found": true, "summary": "...", "root_cause": "...", "suggestion": "..."}}
    """
    
    try:
        llm = GeminiClient()
        response = await asyncio.to_thread(llm.generate_json, messages[-1].content, system_prompt)
        return {"analysis_result": json.dumps(response, ensure_ascii=False), "iteration": iteration + 1}
    except Exception as e:
        logger.error(f"AI ì§„ë‹¨ ì—ëŸ¬: {e}")
        return {"analysis_result": json.dumps({"action": "finish", "analysis": {"error_found": True, "summary": f"ì§„ë‹¨ ì—ëŸ¬: {e}"}})}

async def tool_execution_node(state: LogAnalysisState):
    """
    [ë…¸ë“œ 3: ë„êµ¬ ì‹¤í–‰]
    """
    decision = json.loads(state.get("analysis_result", "{}"))
    action = decision.get("action")
    args = decision.get("args", {})
    
    tool_map = {
        "read_file": execute_read_file,
        "check_db": execute_check_db,
        "check_system": execute_check_system,
        "check_sqs": execute_check_sqs
    }
    
    if action in tool_map:
        result = await asyncio.to_thread(tool_map[action], args)
        feedback = f"[{action} ê²°ê³¼]\n{result}\n\nìœ„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„ì„ ê³„ì†í•˜ê±°ë‚˜ ì¢…ë£Œí•˜ì‹­ì‹œì˜¤."
        
        curr_messages = state.get("messages", [])
        curr_messages.append(AIMessage(content=f"Executed {action}"))
        curr_messages.append(HumanMessage(content=feedback))
        return {"messages": curr_messages}
        
    return {}

def should_continue(state: LogAnalysisState):
    decision = json.loads(state.get("analysis_result", "{}"))
    if decision.get("action") in ["read_file", "check_db", "check_system", "check_sqs"] and state.get("iteration", 0) < 5:
        return "tool_exec"
    return END

# Graph
workflow = StateGraph(LogAnalysisState)
workflow.add_node("fetch", fetch_logs_node)
workflow.add_node("diagnose", diagnose_node)
workflow.add_node("tool_exec", tool_execution_node)

workflow.set_entry_point("fetch")
workflow.add_edge("fetch", "diagnose")
workflow.add_conditional_edges("diagnose", should_continue, {"tool_exec": "tool_exec", END: END})
workflow.add_edge("tool_exec", "diagnose")

app = workflow.compile()
