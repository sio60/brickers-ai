from typing import TypedDict, Annotated, List, Optional, Dict, Any, Union
import os
import docker
import json
import logging
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
from langgraph.graph import StateGraph, END

# Logger configuration
logger = logging.getLogger("agent.log_agent")

# LLM Client Import
try:
    from .llm_clients import GeminiClient
    from .agent_tools import ReadFileSnippet, CheckDBStatus, CheckSystemHealth, CheckSQSStatus, execute_read_file, execute_check_db, execute_check_system, execute_check_sqs
except ImportError:
    # Standalone execution support
    import sys
    from pathlib import Path
    sys.path.append(str(Path(__file__).resolve().parent))
    from llm_clients import GeminiClient
    from agent_tools import ReadFileSnippet, CheckDBStatus, CheckSystemHealth, CheckSQSStatus, execute_read_file, execute_check_db, execute_check_system, execute_check_sqs

# --- State Definition ---
class LogAnalysisState(TypedDict):
    container_name: str
    logs: str  # Raw logs
    messages: List[Annotated[str, "History"]] 
    analysis_result: Optional[str] 
    error_count: int
    iteration: int # Loop counter
    job_id: Optional[str] # íŠ¹ì • Job ì¶”ì ìš© ì¶”ê°€

# --- Tool Execution Logic ---
# --- Tool Execution Logic (Moved to agent_tools.py or imported) ---
# execute_read_file is now imported from agent_tools.py
# ensuring we use the centralized definition.

# --- Nodes ---

# --- Nodes (ë…¸ë“œ ì •ì˜) ---



# --- Nodes (ë…¸ë“œ ì •ì˜) ---

async def fetch_logs_node(state: LogAnalysisState):
    """
    [ë…¸ë“œ 1: ë¡œê·¸ ìˆ˜ì§‘]
    Docker SDK ë˜ëŠ” ìƒíƒœê°’ì„ í™œìš©í•˜ì—¬ íŠ¹ì • Jobì˜ ë¡œê·¸ë§Œ ì¶”ì¶œí•˜ê±°ë‚˜ ìµœê·¼ ì‹¤íŒ¨í•œ Jobì„ ì°¾ìŠµë‹ˆë‹¤.
    """
    container_name = state.get("container_name", "brickers-ai-container")
    target_job_id = state.get("job_id") # íŠ¹ì • Job IDê°€ ì§€ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
    
    logger.info(f"--- [ë¡œê·¸ ì—ì´ì „íŠ¸] 1ë‹¨ê³„: ë¡œê·¸ ìˆ˜ì§‘ ë° Job ë¶„ì„ ì‹œì‘ ({container_name}) ---")
    
    try:
        # 1. ë…ì»¤ì—ì„œ ë„‰ë„‰í•˜ê²Œ ë¡œê·¸ ê°€ì ¸ì˜¤ê¸° (Job ì „ì²´ ë§¥ë½ì„ íŒŒì•…í•˜ê¸° ìœ„í•´)
        client = docker.from_env()
        container = client.containers.get(container_name)
        raw_logs = container.logs(tail=2000).decode("utf-8", errors="replace")
        logger.info(f"âœ… ì›ë³¸ ë¡œê·¸ ìˆ˜ì§‘ ì™„ë£Œ ({len(raw_logs)} ë°”ì´íŠ¸).")
    except Exception as e:
        existing_logs = state.get("logs", "")
        if existing_logs:
             logger.warning("âš ï¸ ë…ì»¤ ì—°ê²° ì‹¤íŒ¨. ì…ë ¥ëœ í…ìŠ¤íŠ¸ ë¡œê·¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
             raw_logs = existing_logs 
        else:
             logger.error(f"âŒ ë…ì»¤ ë¡œê·¸ ìˆ˜ì§‘ ì—ëŸ¬: {str(e)}")
             return {"analysis_result": json.dumps({"action": "finish", "analysis": {"error_found": True, "summary": f"ë¡œê·¸ ìˆ˜ì§‘ ë¶ˆê°€: {str(e)}"}})}

    # 2. Job ID ê¸°ë°˜ ë¡œê·¸ ì¶”ì¶œ ë¡œì§
    # ì‹¤íŒ¨í•œ Job ID ì°¾ê¸° (íŒ¨í„´: âŒ [AI-SERVER] ìš”ì²­ ì‹¤íŒ¨! | jobId=...)
    import re
    
    if not target_job_id:
        # ê°€ì¥ ìµœê·¼ì— 'ì‹¤íŒ¨'í•œ Job IDë¥¼ ì°¾ìŒ
        failure_matches = re.findall(r"ìš”ì²­ ì‹¤íŒ¨! \| jobId=([a-f0-9-]+)", raw_logs)
        if failure_matches:
            target_job_id = failure_matches[-1]
            logger.info(f"ğŸ•µï¸ ìµœê·¼ ì‹¤íŒ¨í•œ Job ë°œê²¬: {target_job_id}")
        else:
            # ì‹¤íŒ¨ê±´ì´ ì—†ìœ¼ë©´ ê°€ì¥ ìµœê·¼ 'ì‹œì‘'ëœ Job ID ì¶”ì¶œ
            start_matches = re.findall(r"ìš”ì²­ ì‹œì‘ \| jobId=([a-f0-9-]+)", raw_logs)
            if start_matches:
                target_job_id = start_matches[-1]
                logger.info(f"â„¹ï¸ ì‹¤íŒ¨ ê±´ì€ ì—†ìœ¼ë‚˜ ìµœê·¼ Job ë¶„ì„ ì§„í–‰: {target_job_id}")

    # 3. í•´ë‹¹ Job IDì™€ ê´€ë ¨ëœ ë¡œê·¸ë§Œ ëª¨ìœ¼ê¸°
    if target_job_id:
        job_logs_list = []
        for line in raw_logs.splitlines():
            if target_job_id in line:
                job_logs_list.append(line)
        
        filtered_logs = "\n".join(job_logs_list)
        logger.info(f"ğŸ“‚ Job [{target_job_id}] ê´€ë ¨ ë¡œê·¸ {len(job_logs_list)}ì¤„ í•„í„°ë§ ì™„ë£Œ.")
    else:
        filtered_logs = raw_logs[-4000:] # Job ID ëª» ì°¾ìœ¼ë©´ ê¸°ì¡´ì²˜ëŸ¼ ë§ˆì§€ë§‰ ë¶€ë¶„ ì‚¬ìš©
        logger.warning("âš ï¸ Job IDë¥¼ ì‹ë³„í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ 4000ìë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    user_prompt = f"""
    [ì§€ì •ëœ Job ID: {target_job_id or "ì•Œ ìˆ˜ ì—†ìŒ"}]
    [í•´ë‹¹ Job ê´€ë ¨ ë¡œê·¸]
    {filtered_logs} 
    
    ì´ Jobì˜ ë¡œê·¸ë§Œ ì§‘ì¤‘ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì˜¤ë¥˜(Traceback, Exception, Timeout)ì˜ ê·¼ë³¸ ì›ì¸ì„ ì‹ë³„í•˜ì‹­ì‹œì˜¤.
    
    ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬:
    1. `read_file`: ì½”ë“œ ë ˆë²¨ì˜ ì˜¤ë¥˜ í™•ì¸ ì‹œ ì‚¬ìš©.
    2. `check_db`: DB ì—°ê²°/ìƒíƒœ ì ê²€ ì‹œ ì‚¬ìš©.
    3. `check_sqs`: ë©”ì‹œì§€ í ì§€ì—°/ì—ëŸ¬ ì‹œ ì‚¬ìš©.
    4. `check_system`: ë¦¬ì†ŒìŠ¤ ë¶€ì¡± ì˜ì‹¬ ì‹œ ì‚¬ìš©.
    """
    
    return {
        "logs": filtered_logs, 
        "messages": [HumanMessage(content=user_prompt)], 
        "iteration": 0,
        "job_id": target_job_id
    }

async def diagnose_node(state: LogAnalysisState):
    """
    [ë…¸ë“œ 2: ì—ëŸ¬ ì§„ë‹¨ ë° ì˜ì‚¬ê²°ì •]
    """
    messages = state.get("messages", [])
    iteration = state.get("iteration", 0)
    logger.info(f"--- [ë¡œê·¸ ì—ì´ì „íŠ¸] 2ë‹¨ê³„: ì—ëŸ¬ ì§„ë‹¨ ì¤‘ (ë°˜ë³µ: {iteration}) ---")
    
    system_prompt = """
    ë‹¹ì‹ ì€ ì „ë¬¸ ë””ë²„ê¹… ì—ì´ì „íŠ¸ ë° ì‹œìŠ¤í…œ ì•„í‚¤í…íŠ¸ì…ë‹ˆë‹¤.
    ëª©í‘œ: íŠ¹ì • Job IDì™€ ê´€ë ¨ëœ ë¡œê·¸ë¥¼ ë¶„ì„í•˜ì—¬ ê·¼ë³¸ ì›ì¸ì„ ì°¾ìœ¼ì‹­ì‹œì˜¤.
    
    ì˜ì‚¬ê²°ì • í”„ë¡œì„¸ìŠ¤:
    1. **ë¡œê·¸ ë¶„ì„**: ì½”ë“œ ì—ëŸ¬(`read_file`), DB(`check_db`), SQS(`check_sqs`), ì‹œìŠ¤í…œ(`check_system`) ì¤‘ ì˜ì‹¬ ì§€ì  í™•ì¸.
    2. **ì•„í‚¤í…ì²˜ ì œì•ˆ**: ë‹¨ìˆœ ìˆ˜ì¹˜ ì¡°ì •ì„ ë„˜ì–´, íŠ¹ì • ë¡œì§ì´ ëˆ„ë½ë˜ì—ˆê±°ë‚˜ ìƒˆë¡œìš´ í•¨ìˆ˜(ë„êµ¬)ê°€ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨ë˜ë©´ ì´ë¥¼ í•´ê²°ì±…(suggestion)ì— êµ¬ì²´ì ì¸ ì½”ë“œ ì˜ˆì‹œì™€ í•¨ê»˜ í¬í•¨í•˜ì‹­ì‹œì˜¤.
    
    ì¶œë ¥ í˜•ì‹ (JSON):
    - ë„êµ¬ ì‚¬ìš©: `{"action": "ë„êµ¬ì´ë¦„", "args": {...}, "reasoning": "ì´ìœ "}`
    - ì¢…ë£Œ: `{"action": "finish", "analysis": {"error_found": true, "summary": "ìš”ì•½", "root_cause": "ì›ì¸", "suggestion": "í•´ê²°ì±… (í•„ìš”ì‹œ ìƒˆë¡œìš´ ë¡œì§/í•¨ìˆ˜ ì„¤ê³„ í¬í•¨)"}}`
    
    ëª¨ë“  ë¶„ì„ ë³´ê³ ì„œëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
    """
    
    try:
        llm = GeminiClient()
        # ë¹„ë™ê¸° í˜¸ì¶œì„ ìœ„í•´ to_thread ì‚¬ìš© (llm_clientsê°€ ë™ê¸°ì¸ ê²½ìš°)
        import asyncio
        response = await asyncio.to_thread(llm.generate_json, messages[-1].content, system_prompt)
        logger.info(f"ğŸ¤– AI ê²°ì •: {json.dumps(response, ensure_ascii=False)}")
        
        return {"analysis_result": json.dumps(response, ensure_ascii=False), "iteration": iteration + 1}
        
    except Exception as e:
         logger.error(f"âŒ AI ì§„ë‹¨ ì‹¤íŒ¨: {str(e)}")
         return {"analysis_result": json.dumps({"action": "finish", "analysis": {"error_found": True, "summary": f"ì§„ë‹¨ ë„ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}"}})}

async def tool_execution_node(state: LogAnalysisState):
    """
    [ë…¸ë“œ 3: ë„êµ¬ ì‹¤í–‰]
    """
    raw_result = state.get("analysis_result")
    try:
        decision = json.loads(raw_result)
    except:
        logger.error("âŒ ë„êµ¬ ì‹¤í–‰ ë…¸ë“œì—ì„œ JSON íŒŒì‹± ì—ëŸ¬ ë°œìƒ.")
        return {"messages": [HumanMessage(content="JSON ì˜ì‚¬ê²°ì • íŒŒì‹± ì—ëŸ¬.")]}

    action = decision.get("action")
    args = decision.get("args", {})
    logger.info(f"ğŸ› ï¸ [ë¡œê·¸ ì—ì´ì „íŠ¸] 3ë‹¨ê³„: ë„êµ¬ '{action}' ì‹¤í–‰ ì¤‘...")
    
    # ë„êµ¬ë³„ ì‹¤í–‰ (í˜„ì¬ ë„êµ¬ë“¤ì€ ë™ê¸° ë°©ì‹ì´ë¯€ë¡œ to_thread ê¶Œì¥)
    import asyncio
    tool_output = ""

    if action == "read_file":
        tool_output = await asyncio.to_thread(execute_read_file, args)
    elif action == "check_db":
        tool_output = await asyncio.to_thread(execute_check_db, args)
    elif action == "check_system":
        tool_output = await asyncio.to_thread(execute_check_system, args)
    elif action == "check_sqs":
        tool_output = await asyncio.to_thread(execute_check_sqs, args)
    
    if tool_output:
        logger.info(f"ğŸ“¥ ë„êµ¬ ê²°ê³¼ ìˆ˜ì‹  ({len(tool_output)} ë°”ì´íŠ¸).")
        # LLMì—ê²Œ ë„êµ¬ ê²°ê³¼ ì „ë‹¬
        feedback_msg = f"""
        [{action} ë„êµ¬ ì‹¤í–‰ ê²°ê³¼]
        {tool_output}
        
        ì´ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê·¼ë³¸ ì›ì¸ì„ íŒŒì•…í–ˆìŠµë‹ˆê¹Œ?
        íŒŒì•…í–ˆë‹¤ë©´ finishë¥¼, ë” ì •ë³´ê°€ í•„ìš”í•˜ë©´ ë‹¤ë¥¸ ë„êµ¬ë¥¼ ìš”ì²­í•˜ì‹­ì‹œì˜¤.
        """
        messages = state.get("messages", [])
        messages.append(AIMessage(content=f"Executed {action}"))
        messages.append(HumanMessage(content=feedback_msg))
        
        return {"messages": messages}
    
    logger.warning("âš ï¸ ë„êµ¬ê°€ ì•„ë¬´ëŸ° ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return {}

# --- Conditional Edge (ë¶„ê¸° ì¡°ê±´) ---
def should_continue(state: LogAnalysisState):
    iteration = state.get("iteration", 0)
    raw_result = state.get("analysis_result")
    
    try:
        decision = json.loads(raw_result)
        action = decision.get("action")
        
        # ë„êµ¬ ì‚¬ìš© ìš”ì²­ì´ê³ , ë°˜ë³µ íšŸìˆ˜ê°€ 5íšŒ ë¯¸ë§Œì´ë©´ ê³„ì† ì§„í–‰
        if action in ["read_file", "check_db", "check_system", "check_sqs"] and iteration < 5: 
            logger.info(f"ğŸ”„ Routing to 'inspect_code' (Current Iteration: {iteration})")
            return "inspect_code"
        else:
            logger.info(f"ğŸ”š Routing to 'END' (Action: {action}, Iteration: {iteration})")
            return END 
    except:
        logger.error("âŒ Error in routing logic, forced to END.")
        return END

# --- Graph Construction (ê·¸ë˜í”„ ì¡°ë¦½) ---
workflow = StateGraph(LogAnalysisState)

# 1. ë…¸ë“œ ì¶”ê°€
workflow.add_node("fetch_logs", fetch_logs_node)      # ë¡œê·¸ ìˆ˜ì§‘
workflow.add_node("diagnose_error", diagnose_node)    # ë¶„ì„ ë° íŒë‹¨
workflow.add_node("inspect_code", tool_execution_node) # ì½”ë“œ ì¡°íšŒ (Tool)

# 2. ì—£ì§€ ì—°ê²° (íë¦„ ì •ì˜)
workflow.set_entry_point("fetch_logs")                # ì‹œì‘ì 
workflow.add_edge("fetch_logs", "diagnose_error")     # ìˆ˜ì§‘ -> ì§„ë‹¨

# 3. ë¶„ê¸° ë° ìˆœí™˜ ì„¤ì •
workflow.add_conditional_edges(
    "diagnose_error",
    should_continue,
    {
        "inspect_code": "inspect_code",  # ì½”ë“œ ë” ë´ì•¼ í•˜ë©´ -> inspect_code
        END: END                         # ë‹¤ ë´¤ìœ¼ë©´ -> ë
    }
)

workflow.add_edge("inspect_code", "diagnose_error") # ì½”ë“œ ë´¤ìœ¼ë©´ ë‹¤ì‹œ ì§„ë‹¨ (Loop Back)

app = workflow.compile()

